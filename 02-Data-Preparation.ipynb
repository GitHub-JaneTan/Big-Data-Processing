{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab95a53-95e6-44d8-bd82-9d92cbeec45b",
   "metadata": {},
   "source": [
    "## 2.2 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c03e835-8dce-4afc-8811-5230128090f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author: Tan Xin Hui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5964406d-5e3e-4926-b56d-a60a7c04379b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Understanding\n",
      "+-----+-------+-----+------------+------+-------+-----+--------+\n",
      "|Carat|Clarity|Color|Fluorescence|Polish|  Price|Shape|Symmetry|\n",
      "+-----+-------+-----+------------+------+-------+-----+--------+\n",
      "| 0.73|   VVS1|   H+|           N|    EX|10500.0|   RD|      EX|\n",
      "| 0.58|   VVS1|   E+|           N|    EX|10749.0|   RD|      EX|\n",
      "| 0.58|   VVS1|   E+|           N|    EX|10749.0|   RD|      EX|\n",
      "| 0.58|   VVS1|   E+|           N|    EX|10749.0|   RD|      EX|\n",
      "| 0.57|   VVS1|   E+|           N|    EX|10250.0|   RD|      EX|\n",
      "+-----+-------+-----+------------+------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2(b) Label Encoding: \n",
      "+-------------+---------------+-------------+--------------+----------------+--------------------+\n",
      "|Shape_encoded|Clarity_encoded|Color_encoded|Polish_encoded|Symmetry_encoded|Fluorescence_encoded|\n",
      "+-------------+---------------+-------------+--------------+----------------+--------------------+\n",
      "|          0.0|            0.0|          8.0|           0.0|             0.0|                 0.0|\n",
      "|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|\n",
      "|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|\n",
      "|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|\n",
      "|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|\n",
      "|          0.0|            0.0|          8.0|           0.0|             0.0|                 0.0|\n",
      "|          0.0|            0.0|          8.0|           0.0|             0.0|                 0.0|\n",
      "|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|\n",
      "|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|\n",
      "|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|\n",
      "+-------------+---------------+-------------+--------------+----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2(c) Data Cleaning\n",
      "Number of rows before removing outliers: 1935\n",
      "Number of rows after removing outliers: 1915\n",
      "2.2(d) Data Transformation\n",
      "+---------------+\n",
      "|Price per Carat|\n",
      "+---------------+\n",
      "|       14383.56|\n",
      "|       18532.76|\n",
      "|       18532.76|\n",
      "|       18532.76|\n",
      "|       17982.46|\n",
      "|       14383.56|\n",
      "|       14383.56|\n",
      "|       17982.46|\n",
      "|       17982.46|\n",
      "|       17982.46|\n",
      "+---------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Final DataFrame with Encoded Columns and Price per Carat:\n",
      "+-----+-------+-----+------------+------+-------+-----+--------+-------------+---------------+-------------+--------------+----------------+--------------------+---------------+\n",
      "|Carat|Clarity|Color|Fluorescence|Polish|  Price|Shape|Symmetry|Shape_encoded|Clarity_encoded|Color_encoded|Polish_encoded|Symmetry_encoded|Fluorescence_encoded|Price per Carat|\n",
      "+-----+-------+-----+------------+------+-------+-----+--------+-------------+---------------+-------------+--------------+----------------+--------------------+---------------+\n",
      "| 0.73|   VVS1|   H+|           N|    EX|10500.0|   RD|      EX|          0.0|            0.0|          8.0|           0.0|             0.0|                 0.0|       14383.56|\n",
      "| 0.58|   VVS1|   E+|           N|    EX|10749.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       18532.76|\n",
      "| 0.58|   VVS1|   E+|           N|    EX|10749.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       18532.76|\n",
      "| 0.58|   VVS1|   E+|           N|    EX|10749.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       18532.76|\n",
      "| 0.57|   VVS1|   E+|           N|    EX|10250.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       17982.46|\n",
      "| 0.73|   VVS1|   H+|           N|    EX|10500.0|   RD|      EX|          0.0|            0.0|          8.0|           0.0|             0.0|                 0.0|       14383.56|\n",
      "| 0.73|   VVS1|   H+|           N|    EX|10500.0|   RD|      EX|          0.0|            0.0|          8.0|           0.0|             0.0|                 0.0|       14383.56|\n",
      "| 0.57|   VVS1|   E+|           N|    EX|10250.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       17982.46|\n",
      "| 0.57|   VVS1|   E+|           N|    EX|10250.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       17982.46|\n",
      "| 0.57|   VVS1|   E+|           N|    EX|10250.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       17982.46|\n",
      "+-----+-------+-----+------------+------+-------+-----+--------+-------------+---------------+-------------+--------------+----------------+--------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:===================================================>    (56 + 4) / 61]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to /user/student/processed_data as csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "class HDFSDataReader:\n",
    "    def __init__(self, hdfs_path: str):\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"ReadHDFS\") \\\n",
    "            .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "            .getOrCreate()\n",
    "        self.hdfs_path = hdfs_path\n",
    "\n",
    "    def read_data(self) -> DataFrame:\n",
    "        self.df = self.spark.read.json(self.hdfs_path)\n",
    "        return self.df\n",
    "\n",
    "    def show_data(self, num_rows=5):\n",
    "        if hasattr(self, 'df'):\n",
    "            print(\"Data Understanding\")\n",
    "            self.df.show(num_rows)\n",
    "        else:\n",
    "            print(\"DataFrame not loaded yet. Call read_data() first.\")\n",
    "\n",
    "    def stop_spark(self):\n",
    "        self.spark.stop()\n",
    "        print(\"Spark session stopped.\")\n",
    "\n",
    "class LabelEncoder:\n",
    "    def __init__(self, dataframe: DataFrame, categorical_columns: list):\n",
    "        self.df = dataframe\n",
    "        self.categorical_columns = categorical_columns\n",
    "\n",
    "    def encode(self) -> DataFrame:\n",
    "        for column in self.categorical_columns:\n",
    "            indexer = StringIndexer(inputCol=column, outputCol=column + '_encoded', handleInvalid=\"keep\")  # Handling nulls\n",
    "            self.df = indexer.fit(self.df).transform(self.df)\n",
    "        \n",
    "        return self.df\n",
    "\n",
    "    def show_encoded_columns(self, num_rows=10):\n",
    "        encoded_columns = [col + '_encoded' for col in self.categorical_columns]\n",
    "        print(\"2.2(b) Label Encoding: \")\n",
    "        self.df.select(encoded_columns).show(num_rows)\n",
    "\n",
    "    def detect_and_remove_outliers(self, column_name: str, z_threshold: float = 3.0) -> DataFrame:\n",
    "        initial_count = self.df.count()\n",
    "        stats = self.df.select(\n",
    "            F.mean(column_name).alias('mean'),\n",
    "            F.stddev(column_name).alias('stddev')\n",
    "        ).collect()[0]\n",
    "        mean = stats['mean']\n",
    "        stddev = stats['stddev']\n",
    "        self.df = self.df.withColumn(\n",
    "            f'{column_name}_z_score',\n",
    "            (F.col(column_name) - mean) / stddev\n",
    "        )\n",
    "        self.df = self.df.filter(F.abs(F.col(f'{column_name}_z_score')) < z_threshold)\n",
    "        self.df = self.df.drop(f'{column_name}_z_score')\n",
    "        final_count = self.df.count()\n",
    "        print(\"2.2(c) Data Cleaning\")\n",
    "        print(f\"Number of rows before removing outliers: {initial_count}\")\n",
    "        print(f\"Number of rows after removing outliers: {final_count}\")\n",
    "        return self.df\n",
    "\n",
    "    def add_price_per_carat(self) -> DataFrame:\n",
    "        self.df = self.df.withColumn(\n",
    "            'Price per Carat',\n",
    "            (F.col('Price') / F.col('Carat'))\n",
    "        ).withColumn(\n",
    "            'Price per Carat',\n",
    "            F.round(F.col('Price per Carat'), 2)\n",
    "        )\n",
    "        return self.df\n",
    "\n",
    "    def show_price_per_carat(self, num_rows=10):\n",
    "        print(\"2.2(d) Data Transformation\")\n",
    "        self.df.select('Price per Carat').show(num_rows)\n",
    "\n",
    "    def show_final_table(self, num_rows=10):\n",
    "        columns_to_show = self.df.columns\n",
    "        print(\"Final DataFrame with Encoded Columns and Price per Carat:\")\n",
    "        self.df.select(columns_to_show).show(num_rows)\n",
    "\n",
    "class HDFSDataSaver:\n",
    "    def __init__(self, output_path: str, file_format: str = 'csv', mode: str = 'overwrite'):\n",
    "        self.output_path = output_path\n",
    "        self.file_format = file_format\n",
    "        self.mode = mode\n",
    "\n",
    "    def save(self, dataframe: DataFrame):\n",
    "        try:\n",
    "            if self.file_format == 'csv':\n",
    "                # Add header=True to save column names\n",
    "                dataframe.write.format(self.file_format).option(\"header\", \"true\").mode(self.mode).save(self.output_path)\n",
    "            else:\n",
    "                dataframe.write.format(self.file_format).mode(self.mode).save(self.output_path)\n",
    "            \n",
    "            print(f\"Data saved to {self.output_path} as {self.file_format}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error while saving DataFrame: {e}\")\n",
    "\n",
    "    def set_file_format(self, file_format: str):\n",
    "        self.file_format = file_format\n",
    "\n",
    "    def set_output_path(self, output_path: str):\n",
    "        self.output_path = output_path\n",
    "\n",
    "    def set_write_mode(self, mode: str):\n",
    "        self.mode = mode\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    hdfs_path = \"/user/student/de-dir\"  # Set the file path to read from\n",
    "    hdfs_reader = HDFSDataReader(hdfs_path)\n",
    "    df = hdfs_reader.read_data()\n",
    "    \n",
    "    if df.count() > 0:\n",
    "        hdfs_reader.show_data(5)\n",
    "\n",
    "        encoder = LabelEncoder(dataframe=df, categorical_columns=['Shape', 'Clarity', 'Color', 'Polish', 'Symmetry', 'Fluorescence'])\n",
    "        df_encoded = encoder.encode()\n",
    "\n",
    "        encoder.show_encoded_columns(10)\n",
    "        df_cleaned = encoder.detect_and_remove_outliers('Price', z_threshold=3.0)\n",
    "        df_with_price_per_carat = encoder.add_price_per_carat()\n",
    "        encoder.show_price_per_carat(10)        \n",
    "        encoder.show_final_table(10)\n",
    "        \n",
    "        saver = HDFSDataSaver(output_path=\"/user/student/processed_data\", file_format='csv', mode='overwrite')\n",
    "        saver.save(df_with_price_per_carat)\n",
    "    else:\n",
    "        print(\"No data found in the specified HDFS path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f97b56-ad88-4086-ae92-51f22c9dd98f",
   "metadata": {},
   "source": [
    "## Save to HBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab273fde-9d2a-4d4d-91b0-3a5e71bfa5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaning\n",
      "Number of rows before removing outliers: 1935\n",
      "Number of rows after removing outliers: 1931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to HBase table processed_data_hbase successfully.\n",
      "HBase connection closed.\n"
     ]
    }
   ],
   "source": [
    "import happybase\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "class HDFSDataProcessor:\n",
    "    def __init__(self, hdfs_path: str):\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"HDFSDataProcessor\") \\\n",
    "            .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "            .getOrCreate()\n",
    "        self.hdfs_path = hdfs_path\n",
    "\n",
    "    def read_data(self) -> DataFrame:\n",
    "        self.df = self.spark.read.json(self.hdfs_path)\n",
    "        return self.df\n",
    "\n",
    "    def encode_labels(self, categorical_columns: list) -> DataFrame:\n",
    "        for column in categorical_columns:\n",
    "            indexer = StringIndexer(inputCol=column, outputCol=column + '_encoded', handleInvalid=\"keep\")\n",
    "            self.df = indexer.fit(self.df).transform(self.df)\n",
    "        return self.df\n",
    "\n",
    "    def detect_and_remove_outliers(self, column_name: str, z_threshold: float = 3.0) -> DataFrame:\n",
    "        initial_count = self.df.count()\n",
    "        stats = self.df.select(\n",
    "            F.mean(column_name).alias('mean'),\n",
    "            F.stddev(column_name).alias('stddev')\n",
    "        ).collect()[0]\n",
    "        mean = stats['mean']\n",
    "        stddev = stats['stddev']\n",
    "        self.df = self.df.withColumn(\n",
    "            f'{column_name}_z_score',\n",
    "            (F.col(column_name) - mean) / stddev\n",
    "        )\n",
    "        self.df = self.df.filter(F.abs(F.col(f'{column_name}_z_score')) < z_threshold)\n",
    "        self.df = self.df.drop(f'{column_name}_z_score')\n",
    "        final_count = self.df.count()\n",
    "        print(\"Data Cleaning\")\n",
    "        print(f\"Number of rows before removing outliers: {initial_count}\")\n",
    "        print(f\"Number of rows after removing outliers: {final_count}\")\n",
    "        return self.df\n",
    "\n",
    "    def add_price_per_carat(self) -> DataFrame:\n",
    "        self.df = self.df.withColumn(\n",
    "            'Price per Carat',\n",
    "            (F.col('Price') / F.col('Carat'))\n",
    "        ).withColumn(\n",
    "            'Price per Carat',\n",
    "            F.round(F.col('Price per Carat'), 2)\n",
    "        )\n",
    "        return self.df\n",
    "\n",
    "    def stop_spark(self):\n",
    "        self.spark.stop()\n",
    "        print(\"Spark session stopped.\")\n",
    "\n",
    "class HBaseDataSaver:\n",
    "    def __init__(self, table_name: str, hbase_host: str = 'localhost'):\n",
    "        self.table_name = table_name\n",
    "        self.hbase_host = hbase_host\n",
    "        self.connection = happybase.Connection(self.hbase_host)\n",
    "        self.table = self.connection.table(self.table_name)\n",
    "\n",
    "    def save(self, dataframe: DataFrame):\n",
    "        try:\n",
    "            dataframe = dataframe.withColumn('row_key', monotonically_increasing_id())\n",
    "            row_key_col = 'row_key'\n",
    "            columns = dataframe.columns\n",
    "            \n",
    "            for row in dataframe.collect():\n",
    "                row_dict = row.asDict()\n",
    "                row_key = str(row_dict.pop(row_key_col))\n",
    "                data = {f'cf:{col}': str(value) for col, value in row_dict.items() if value is not None}\n",
    "                self.table.put(row_key, data)\n",
    "            print(f\"Data saved to HBase table {self.table_name} successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error while saving data to HBase: {e}\")\n",
    "            print(f\"DataFrame columns: {dataframe.columns}\")\n",
    "\n",
    "    def close(self):\n",
    "        try:\n",
    "            self.connection.close()\n",
    "            print(\"HBase connection closed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error while closing HBase connection: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # HDFS path\n",
    "        hdfs_path = \"/user/student/de-dir\"\n",
    "\n",
    "        # Initialize HDFS Data Processor\n",
    "        processor = HDFSDataProcessor(hdfs_path)\n",
    "        df = processor.read_data()\n",
    "\n",
    "        if df.count() > 0:\n",
    "            # Perform preprocessing\n",
    "            categorical_columns = ['Shape', 'Clarity', 'Color', 'Polish', 'Symmetry', 'Fluorescence']\n",
    "            df_encoded = processor.encode_labels(categorical_columns)\n",
    "            df_cleaned = processor.detect_and_remove_outliers('Carat')\n",
    "            df_with_price_per_carat = processor.add_price_per_carat()\n",
    "\n",
    "            # Initialize HBase Data Saver\n",
    "            hbase_saver = HBaseDataSaver(table_name='processed_data_hbase')\n",
    "            hbase_saver.save(df_with_price_per_carat)\n",
    "            hbase_saver.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in processing and saving data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed974e7d-ae50-4e9d-a0d8-9cca4b36dc60",
   "metadata": {},
   "source": [
    "### HBase Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e44d78c5-85e7-48ee-a662-ff23b44ff307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/10 21:57:36 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------------------+--------+----------------+---------------+-----------------------+---------+-----------------+--------+------------------+--------+----------------+-----------+-------------------+------------+\n",
      "|cf:Carat|cf:Clarity|cf:Clarity_encoded|cf:Color|cf:Color_encoded|cf:Fluorescence|cf:Fluorescence_encoded|cf:Polish|cf:Polish_encoded|cf:Price|cf:Price per Carat|cf:Shape|cf:Shape_encoded|cf:Symmetry|cf:Symmetry_encoded|     row_key|\n",
      "+--------+----------+------------------+--------+----------------+---------------+-----------------------+---------+-----------------+--------+------------------+--------+----------------+-----------+-------------------+------------+\n",
      "|    0.73|      VVS1|               0.0|      H+|             8.0|              N|                    0.0|       EX|              0.0| 10500.0|          14383.56|      RD|             0.0|         EX|                0.0|           0|\n",
      "|    0.58|      VVS1|               0.0|      E+|             3.0|              N|                    0.0|       EX|              0.0| 10749.0|          18532.76|      RD|             0.0|         EX|                0.0|           1|\n",
      "|    0.73|      VVS1|               0.0|      H+|             8.0|              N|                    0.0|       EX|              0.0| 10500.0|          14383.56|      RD|             0.0|         EX|                0.0|          10|\n",
      "|    0.35|      VVS1|               0.0|       E|             1.0|              N|                    0.0|       EX|              0.0|  3866.0|          11045.71|      RD|             0.0|         EX|                0.0|103079215104|\n",
      "|    0.31|      VVS1|               0.0|       E|             1.0|              N|                    0.0|       EX|              0.0|  3220.0|           10387.1|      RD|             0.0|         EX|                0.0|103079215105|\n",
      "+--------+----------+------------------+--------+----------------+---------------+-----------------------+---------+-----------------+--------+------------------+--------+----------------+-----------+-------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "HBase connection closed.\n"
     ]
    }
   ],
   "source": [
    "import happybase\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import Row\n",
    "\n",
    "class HBaseDataReader:\n",
    "    def __init__(self, table_name: str, hbase_host: str = 'localhost'):\n",
    "        self.table_name = table_name\n",
    "        self.hbase_host = hbase_host\n",
    "        self.connection = happybase.Connection(self.hbase_host)\n",
    "        self.table = self.connection.table(self.table_name)\n",
    "    \n",
    "    def read_data(self):\n",
    "        # Scan the table and collect data\n",
    "        rows = self.table.scan()\n",
    "        data = []\n",
    "        for key, row in rows:\n",
    "            # Convert each row to a dictionary\n",
    "            row_dict = {col.decode('utf-8'): val.decode('utf-8') for col, val in row.items()}\n",
    "            row_dict['row_key'] = key.decode('utf-8')\n",
    "            data.append(row_dict)\n",
    "        return data\n",
    "    \n",
    "    def to_dataframe(self, data: list) -> DataFrame:\n",
    "        # Create a SparkSession\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"HBase to DataFrame\") \\\n",
    "            .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        # Convert the list of dictionaries to a DataFrame\n",
    "        df = spark.createDataFrame([Row(**row) for row in data])\n",
    "        return df\n",
    "    \n",
    "    def show_data(self, num_rows=5):\n",
    "        # Read and convert data to DataFrame\n",
    "        data = self.read_data()\n",
    "        df = self.to_dataframe(data)\n",
    "        # Show DataFrame\n",
    "        df.show(num_rows)\n",
    "\n",
    "    def close(self):\n",
    "        self.connection.close()\n",
    "        print(\"HBase connection closed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Initialize HBaseDataReader\n",
    "        hbase_reader = HBaseDataReader(table_name='processed_data_hbase')\n",
    "        \n",
    "        # Show some rows from HBase\n",
    "        hbase_reader.show_data(5)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in HBase reader: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Ensure connection is closed\n",
    "        hbase_reader.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943d4e04-6740-49aa-b6d2-bd7c577da192",
   "metadata": {},
   "source": [
    "### HBase Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4f2cce1-4b6a-4240-ae51-ffe9b9c081d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Families:\n",
      "Family: cf, Attributes: {'name': b'cf:', 'max_versions': 1, 'compression': b'NONE', 'in_memory': False, 'bloom_filter_type': b'ROW', 'bloom_filter_vector_size': 0, 'bloom_filter_nb_hashes': 0, 'block_cache_enabled': True, 'time_to_live': 2147483647}\n"
     ]
    }
   ],
   "source": [
    "import happybase\n",
    "\n",
    "def describe_table(hbase_host, table_name):\n",
    "    connection = happybase.Connection(hbase_host)\n",
    "    table = connection.table(table_name)\n",
    "    families = table.families()\n",
    "    print(\"Column Families:\")\n",
    "    for family, attributes in families.items():\n",
    "        print(f\"Family: {family.decode('utf-8')}, Attributes: {attributes}\")\n",
    "    connection.close()\n",
    "\n",
    "# Call the function\n",
    "describe_table('localhost', 'processed_data_hbase')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5083d231-f752-4998-b485-2cce45cfe97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{b'cf:Carat': b'0.73', b'cf:Clarity': b'VVS1', b'cf:Clarity_encoded': b'0.0', b'cf:Color': b'H+', b'cf:Color_encoded': b'8.0', b'cf:Fluorescence': b'N', b'cf:Fluorescence_encoded': b'0.0', b'cf:Polish': b'EX', b'cf:Polish_encoded': b'0.0', b'cf:Price': b'10500.0', b'cf:Price per Carat': b'14383.56', b'cf:Shape': b'RD', b'cf:Shape_encoded': b'0.0', b'cf:Symmetry': b'EX', b'cf:Symmetry_encoded': b'0.0'}\n"
     ]
    }
   ],
   "source": [
    "# Query 1: Get Specific Row: Retrieve a specific row by its row key.\n",
    "\n",
    "connection = happybase.Connection('localhost')\n",
    "table = connection.table('processed_data_hbase')\n",
    "\n",
    "row = table.row(b'0')\n",
    "print(row)\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "727feeb3-acf6-4105-8e24-dd106260368c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{b'cf:Carat': b'0.73'}\n"
     ]
    }
   ],
   "source": [
    "# Get Specific Columns: Retrieve specific columns for a row.\n",
    "\n",
    "connection = happybase.Connection('localhost')\n",
    "table = connection.table('processed_data_hbase')\n",
    "\n",
    "columns = [b'cf:Carat']\n",
    "row = table.row(b'0', columns=columns)\n",
    "print(row)\n",
    "\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b955631-bf58-4dcf-850e-4b1b60c55a20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row key: 103079215104\n",
      "  cf:Carat: 0.35\n",
      "  cf:Clarity: VVS1\n",
      "  cf:Clarity_encoded: 0.0\n",
      "  cf:Color: E\n",
      "  cf:Color_encoded: 1.0\n",
      "  cf:Fluorescence: N\n",
      "  cf:Fluorescence_encoded: 0.0\n",
      "  cf:Polish: EX\n",
      "  cf:Polish_encoded: 0.0\n",
      "  cf:Price: 3866.0\n",
      "  cf:Price per Carat: 11045.71\n",
      "  cf:Shape: RD\n",
      "  cf:Shape_encoded: 0.0\n",
      "  cf:Symmetry: EX\n",
      "  cf:Symmetry_encoded: 0.0\n",
      "\n",
      "Row key: 103079215105\n",
      "  cf:Carat: 0.31\n",
      "  cf:Clarity: VVS1\n",
      "  cf:Clarity_encoded: 0.0\n",
      "  cf:Color: E\n",
      "  cf:Color_encoded: 1.0\n",
      "  cf:Fluorescence: N\n",
      "  cf:Fluorescence_encoded: 0.0\n",
      "  cf:Polish: EX\n",
      "  cf:Polish_encoded: 0.0\n",
      "  cf:Price: 3220.0\n",
      "  cf:Price per Carat: 10387.1\n",
      "  cf:Shape: RD\n",
      "  cf:Shape_encoded: 0.0\n",
      "  cf:Symmetry: EX\n",
      "  cf:Symmetry_encoded: 0.0\n",
      "\n",
      "Row key: 103079215106\n",
      "  cf:Carat: 0.37\n",
      "  cf:Clarity: VVS2\n",
      "  cf:Clarity_encoded: 1.0\n",
      "  cf:Color: D\n",
      "  cf:Color_encoded: 0.0\n",
      "  cf:Fluorescence: F\n",
      "  cf:Fluorescence_encoded: 1.0\n",
      "  cf:Polish: EX\n",
      "  cf:Polish_encoded: 0.0\n",
      "  cf:Price: 3184.0\n",
      "  cf:Price per Carat: 8605.41\n",
      "  cf:Shape: RD\n",
      "  cf:Shape_encoded: 0.0\n",
      "  cf:Symmetry: EX\n",
      "  cf:Symmetry_encoded: 0.0\n",
      "\n",
      "Row key: 103079215107\n",
      "  cf:Carat: 0.5\n",
      "  cf:Clarity: VVS1\n",
      "  cf:Clarity_encoded: 0.0\n",
      "  cf:Color: F+\n",
      "  cf:Color_encoded: 7.0\n",
      "  cf:Fluorescence: N\n",
      "  cf:Fluorescence_encoded: 0.0\n",
      "  cf:Polish: EX\n",
      "  cf:Polish_encoded: 0.0\n",
      "  cf:Price: 7719.0\n",
      "  cf:Price per Carat: 15438.0\n",
      "  cf:Shape: RD\n",
      "  cf:Shape_encoded: 0.0\n",
      "  cf:Symmetry: EX\n",
      "  cf:Symmetry_encoded: 0.0\n",
      "\n",
      "Row key: 103079215108\n",
      "  cf:Carat: 0.5\n",
      "  cf:Clarity: VVS1\n",
      "  cf:Clarity_encoded: 0.0\n",
      "  cf:Color: E+\n",
      "  cf:Color_encoded: 3.0\n",
      "  cf:Fluorescence: N\n",
      "  cf:Fluorescence_encoded: 0.0\n",
      "  cf:Polish: EX\n",
      "  cf:Polish_encoded: 0.0\n",
      "  cf:Price: 8603.0\n",
      "  cf:Price per Carat: 17206.0\n",
      "  cf:Shape: RD\n",
      "  cf:Shape_encoded: 0.0\n",
      "  cf:Symmetry: EX\n",
      "  cf:Symmetry_encoded: 0.0\n",
      "\n",
      "Row key: 103079215109\n",
      "  cf:Carat: 0.34\n",
      "  cf:Clarity: VVS1\n",
      "  cf:Clarity_encoded: 0.0\n",
      "  cf:Color: F\n",
      "  cf:Color_encoded: 2.0\n",
      "  cf:Fluorescence: N\n",
      "  cf:Fluorescence_encoded: 0.0\n",
      "  cf:Polish: VG\n",
      "  cf:Polish_encoded: 1.0\n",
      "  cf:Price: 3407.0\n",
      "  cf:Price per Carat: 10020.59\n",
      "  cf:Shape: RD\n",
      "  cf:Shape_encoded: 0.0\n",
      "  cf:Symmetry: EX\n",
      "  cf:Symmetry_encoded: 0.0\n",
      "\n",
      "Row key: 103079215110\n",
      "  cf:Carat: 0.5\n",
      "  cf:Clarity: VVS2\n",
      "  cf:Clarity_encoded: 1.0\n",
      "  cf:Color: G+\n",
      "  cf:Color_encoded: 6.0\n",
      "  cf:Fluorescence: F\n",
      "  cf:Fluorescence_encoded: 1.0\n",
      "  cf:Polish: EX\n",
      "  cf:Polish_encoded: 0.0\n",
      "  cf:Price: 4971.0\n",
      "  cf:Price per Carat: 9942.0\n",
      "  cf:Shape: RD\n",
      "  cf:Shape_encoded: 0.0\n",
      "  cf:Symmetry: EX\n",
      "  cf:Symmetry_encoded: 0.0\n",
      "\n",
      "Row key: 103079215111\n",
      "  cf:Carat: 0.45\n",
      "  cf:Clarity: VS1\n",
      "  cf:Clarity_encoded: 3.0\n",
      "  cf:Color: J+\n",
      "  cf:Color_encoded: 11.0\n",
      "  cf:Fluorescence: N\n",
      "  cf:Fluorescence_encoded: 0.0\n",
      "  cf:Polish: EX\n",
      "  cf:Polish_encoded: 0.0\n",
      "  cf:Price: 2792.0\n",
      "  cf:Price per Carat: 6204.44\n",
      "  cf:Shape: RD\n",
      "  cf:Shape_encoded: 0.0\n",
      "  cf:Symmetry: EX\n",
      "  cf:Symmetry_encoded: 0.0\n",
      "\n",
      "Row key: 103079215112\n",
      "  cf:Carat: 0.46\n",
      "  cf:Clarity: VVS2\n",
      "  cf:Clarity_encoded: 1.0\n",
      "  cf:Color: E\n",
      "  cf:Color_encoded: 1.0\n",
      "  cf:Fluorescence: N\n",
      "  cf:Fluorescence_encoded: 0.0\n",
      "  cf:Polish: EX\n",
      "  cf:Polish_encoded: 0.0\n",
      "  cf:Price: 4920.0\n",
      "  cf:Price per Carat: 10695.65\n",
      "  cf:Shape: RD\n",
      "  cf:Shape_encoded: 0.0\n",
      "  cf:Symmetry: EX\n",
      "  cf:Symmetry_encoded: 0.0\n",
      "\n",
      "Row key: 103079215113\n",
      "  cf:Carat: 0.52\n",
      "  cf:Clarity: VVS1\n",
      "  cf:Clarity_encoded: 0.0\n",
      "  cf:Color: E\n",
      "  cf:Color_encoded: 1.0\n",
      "  cf:Fluorescence: N\n",
      "  cf:Fluorescence_encoded: 0.0\n",
      "  cf:Polish: EX\n",
      "  cf:Polish_encoded: 0.0\n",
      "  cf:Price: 8743.0\n",
      "  cf:Price per Carat: 16813.46\n",
      "  cf:Shape: RD\n",
      "  cf:Shape_encoded: 0.0\n",
      "  cf:Symmetry: EX\n",
      "  cf:Symmetry_encoded: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import happybase\n",
    "\n",
    "# Establish connection to HBase\n",
    "connection = happybase.Connection('localhost')\n",
    "table = connection.table('processed_data_hbase')\n",
    "\n",
    "# Define the filter\n",
    "filter = 'SingleColumnValueFilter(' \\\n",
    "          \"'cf', 'Price', >, 'binary:2000')\"\n",
    "\n",
    "# Scan the table with the filter and limit the number of rows returned\n",
    "rows = table.scan(filter=filter, limit=10)\n",
    "\n",
    "# Print the results\n",
    "for key, data in rows:\n",
    "    print(f\"Row key: {key.decode('utf-8')}\")\n",
    "    for column, value in data.items():\n",
    "        print(f\"  {column.decode('utf-8')}: {value.decode('utf-8')}\")\n",
    "    print()\n",
    "\n",
    "# Close the connection\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e45e8da0-e490-4a5e-b002-abb717ac2c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1931\n"
     ]
    }
   ],
   "source": [
    "# Count Rows:\n",
    "\n",
    "connection = happybase.Connection('localhost')\n",
    "table = connection.table('processed_data_hbase')\n",
    "\n",
    "count = 0\n",
    "for _ in table.scan():\n",
    "    count += 1\n",
    "\n",
    "print(f\"Number of rows: {count}\")\n",
    "\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33496370-20b8-4626-bcbc-e5c56088f668",
   "metadata": {},
   "source": [
    "### HDFS Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ae8913e4-9545-4614-b6b2-11bff00762d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Data\n",
      "+-----+-------+-----+------------+------+-------+-----+--------+-------------+---------------+-------------+--------------+----------------+--------------------+---------------+\n",
      "|Carat|Clarity|Color|Fluorescence|Polish|  Price|Shape|Symmetry|Shape_encoded|Clarity_encoded|Color_encoded|Polish_encoded|Symmetry_encoded|Fluorescence_encoded|Price per Carat|\n",
      "+-----+-------+-----+------------+------+-------+-----+--------+-------------+---------------+-------------+--------------+----------------+--------------------+---------------+\n",
      "| 0.73|   VVS1|   H+|           N|    EX|10500.0|   RD|      EX|          0.0|            0.0|          8.0|           0.0|             0.0|                 0.0|       14383.56|\n",
      "| 0.58|   VVS1|   E+|           N|    EX|10749.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       18532.76|\n",
      "| 0.58|   VVS1|   E+|           N|    EX|10749.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       18532.76|\n",
      "| 0.58|   VVS1|   E+|           N|    EX|10749.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       18532.76|\n",
      "| 0.57|   VVS1|   E+|           N|    EX|10250.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       17982.46|\n",
      "+-----+-------+-----+------------+------+-------+-----+--------+-------------+---------------+-------------+--------------+----------------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check saved processed_data dataset \n",
    "class HDFSDataReader:\n",
    "    def __init__(self, hdfs_path: str):\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"ReadHDFS\") \\\n",
    "            .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "            .getOrCreate()\n",
    "        self.hdfs_path = hdfs_path\n",
    "\n",
    "    def read_data(self) -> DataFrame:\n",
    "        self.df = self.spark.read.json(self.hdfs_path)\n",
    "        return self.df\n",
    "    \n",
    "    def read_csv(self, path: str) -> DataFrame:\n",
    "        self.df = self.spark.read.csv(path, header=True, inferSchema=True)\n",
    "        return self.df\n",
    "\n",
    "    def show_data(self, num_rows=5):\n",
    "        if hasattr(self, 'df'):\n",
    "            print(\"Preprocessed Data\")\n",
    "            self.df.show(num_rows)\n",
    "        else:\n",
    "            print(\"DataFrame not loaded yet. Call read_data() or read_csv() first.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hdfs_reader = HDFSDataReader(\"/user/student/processed_data\")\n",
    "    df_saved = hdfs_reader.read_csv(\"/user/student/processed_data\")\n",
    "    hdfs_reader.show_data(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce9e1c56-80ee-45d5-9019-c4b51af5fd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Carat: double (nullable = true)\n",
      " |-- Clarity: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Fluorescence: string (nullable = true)\n",
      " |-- Polish: string (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Shape: string (nullable = true)\n",
      " |-- Symmetry: string (nullable = true)\n",
      " |-- Shape_encoded: double (nullable = true)\n",
      " |-- Clarity_encoded: double (nullable = true)\n",
      " |-- Color_encoded: double (nullable = true)\n",
      " |-- Polish_encoded: double (nullable = true)\n",
      " |-- Symmetry_encoded: double (nullable = true)\n",
      " |-- Fluorescence_encoded: double (nullable = true)\n",
      " |-- Price per Carat: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_saved.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0130da-9a90-4aa9-af30-01424d0f4c85",
   "metadata": {},
   "source": [
    "## 2.3 Data Annotation from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "67cb75e4-b119-4748-9c3a-d4e4e45c59f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/10 22:07:24 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+------------+------+-------+-----+--------+-------------+---------------+-------------+--------------+----------------+--------------------+---------------+-----------+\n",
      "|Carat|Clarity|Color|Fluorescence|Polish|  Price|Shape|Symmetry|Shape_encoded|Clarity_encoded|Color_encoded|Polish_encoded|Symmetry_encoded|Fluorescence_encoded|Price per Carat|price_label|\n",
      "+-----+-------+-----+------------+------+-------+-----+--------+-------------+---------------+-------------+--------------+----------------+--------------------+---------------+-----------+\n",
      "| 0.73|   VVS1|   H+|           N|    EX|10500.0|   RD|      EX|          0.0|            0.0|          8.0|           0.0|             0.0|                 0.0|       14383.56|          2|\n",
      "| 0.58|   VVS1|   E+|           N|    EX|10749.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       18532.76|          2|\n",
      "| 0.58|   VVS1|   E+|           N|    EX|10749.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       18532.76|          2|\n",
      "| 0.58|   VVS1|   E+|           N|    EX|10749.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       18532.76|          2|\n",
      "| 0.57|   VVS1|   E+|           N|    EX|10250.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       17982.46|          2|\n",
      "| 0.73|   VVS1|   H+|           N|    EX|10500.0|   RD|      EX|          0.0|            0.0|          8.0|           0.0|             0.0|                 0.0|       14383.56|          2|\n",
      "| 0.73|   VVS1|   H+|           N|    EX|10500.0|   RD|      EX|          0.0|            0.0|          8.0|           0.0|             0.0|                 0.0|       14383.56|          2|\n",
      "| 0.57|   VVS1|   E+|           N|    EX|10250.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       17982.46|          2|\n",
      "| 0.57|   VVS1|   E+|           N|    EX|10250.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       17982.46|          2|\n",
      "| 0.57|   VVS1|   E+|           N|    EX|10250.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       17982.46|          2|\n",
      "+-----+-------+-----+------------+------+-------+-----+--------+-------------+---------------+-------------+--------------+----------------+--------------------+---------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Data saved to HDFS at /user/student/annotated_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "class DataAnnotator:\n",
    "    def __init__(self, hdfs_path: str):\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"DataAnnotatorApp\") \\\n",
    "            .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        # Read data from HDFS with header information\n",
    "        self.df = self.spark.read.option(\"header\", \"true\").csv(hdfs_path)\n",
    "    \n",
    "    def annotate_price(self, high_threshold: float, low_threshold: float):\n",
    "        if self.df is not None:\n",
    "            for column in self.df.columns:\n",
    "                if column.endswith('_encoded'):\n",
    "                    self.df = self.df.withColumn(column, col(column).cast('double'))\n",
    "        \n",
    "            self.df = self.df.withColumn(\n",
    "                'price_label',\n",
    "                when(col('Price').cast('double') > high_threshold, 2)\n",
    "                .when(col('Price').cast('double') < low_threshold, 0)\n",
    "                .otherwise(1)\n",
    "            )\n",
    "        else:\n",
    "            print(\"DataFrame is not initialized.\")\n",
    "        return self.df\n",
    "\n",
    "    def show_data(self, num_rows=10):\n",
    "        if self.df is not None:\n",
    "            self.df.show(num_rows)\n",
    "        else:\n",
    "            print(\"DataFrame is not initialized.\")\n",
    "    \n",
    "    def close(self):\n",
    "        if self.spark:\n",
    "            self.spark.stop()\n",
    "            print(\"Spark session stopped.\")\n",
    "        else:\n",
    "            print(\"Spark session is not initialized.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hdfs_path = \"/user/student/processed_data\" #set file path to read from \n",
    "\n",
    "    annotator = DataAnnotator(hdfs_path)\n",
    "    \n",
    "    # thresholds for annotation\n",
    "    high_threshold = 10000.0\n",
    "    low_threshold = 5000.0\n",
    "    \n",
    "    # Annotate data\n",
    "    df_annotated = annotator.annotate_price(high_threshold, low_threshold)\n",
    "    annotator.show_data()\n",
    "\n",
    "    #Save to HDFS \n",
    "    saver = HDFSDataSaver(output_path=\"/user/student/annotated_data\", file_format='csv', mode='overwrite')\n",
    "    saver.save(df_annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbc60d69-0e7a-493a-9b91-ec82c6a2ae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Carat: string (nullable = true)\n",
      " |-- Clarity: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Fluorescence: string (nullable = true)\n",
      " |-- Polish: string (nullable = true)\n",
      " |-- Price: string (nullable = true)\n",
      " |-- Shape: string (nullable = true)\n",
      " |-- Symmetry: string (nullable = true)\n",
      " |-- Shape_encoded: double (nullable = true)\n",
      " |-- Clarity_encoded: double (nullable = true)\n",
      " |-- Color_encoded: double (nullable = true)\n",
      " |-- Polish_encoded: double (nullable = true)\n",
      " |-- Symmetry_encoded: double (nullable = true)\n",
      " |-- Fluorescence_encoded: double (nullable = true)\n",
      " |-- Price per Carat: string (nullable = true)\n",
      " |-- price_label: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_annotated.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4191af7f-9a7b-45e2-8db4-599062eeb49b",
   "metadata": {},
   "source": [
    "### HDFS Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "751e4c33-54be-4af9-8d61-d2224b3347c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/10 22:07:10 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---+---+---+-------+---+---+----+----+---+-----+-----+-----+--------+---+\n",
      "|0.73|VVS1| H+|  N|EX4|10500.0| RD|EX7|0.08|0.09|8.0|0.011|0.012|0.013|14383.56|  2|\n",
      "+----+----+---+---+---+-------+---+---+----+----+---+-----+-----+-----+--------+---+\n",
      "|0.58|VVS1| E+|  N| EX|10749.0| RD| EX| 0.0| 0.0|3.0|  0.0|  0.0|  0.0|18532.76|  2|\n",
      "|0.58|VVS1| E+|  N| EX|10749.0| RD| EX| 0.0| 0.0|3.0|  0.0|  0.0|  0.0|18532.76|  2|\n",
      "|0.58|VVS1| E+|  N| EX|10749.0| RD| EX| 0.0| 0.0|3.0|  0.0|  0.0|  0.0|18532.76|  2|\n",
      "|0.57|VVS1| E+|  N| EX|10250.0| RD| EX| 0.0| 0.0|3.0|  0.0|  0.0|  0.0|17982.46|  2|\n",
      "|0.73|VVS1| H+|  N| EX|10500.0| RD| EX| 0.0| 0.0|8.0|  0.0|  0.0|  0.0|14383.56|  2|\n",
      "+----+----+---+---+---+-------+---+---+----+----+---+-----+-----+-----+--------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- 0.73: string (nullable = true)\n",
      " |-- VVS1: string (nullable = true)\n",
      " |-- H+: string (nullable = true)\n",
      " |-- N: string (nullable = true)\n",
      " |-- EX4: string (nullable = true)\n",
      " |-- 10500.0: string (nullable = true)\n",
      " |-- RD: string (nullable = true)\n",
      " |-- EX7: string (nullable = true)\n",
      " |-- 0.08: string (nullable = true)\n",
      " |-- 0.09: string (nullable = true)\n",
      " |-- 8.0: string (nullable = true)\n",
      " |-- 0.011: string (nullable = true)\n",
      " |-- 0.012: string (nullable = true)\n",
      " |-- 0.013: string (nullable = true)\n",
      " |-- 14383.56: string (nullable = true)\n",
      " |-- 2: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/10 22:07:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 0.73, VVS1, H+, N, EX, 10500.0, RD, EX, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 14383.56, 2\n",
      " Schema: 0.73, VVS1, H+, N, EX4, 10500.0, RD, EX7, 0.08, 0.09, 8.0, 0.011, 0.012, 0.013, 14383.56, 2\n",
      "Expected: EX4 but found: EX\n",
      "CSV file: hdfs://localhost:9000/user/student/annotated_data/part-00000-a1004098-63d2-409a-b980-5def7b925555-c000.csv\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Shape` cannot be resolved. Did you mean one of the following? [`2`, `8.0`, `EX4`, `EX7`, `H+`].; line 2 pos 7;\n'Sort ['Shape ASC NULLS FIRST], true\n+- 'Aggregate ['Shape], ['Shape, 'MAX('Carat) AS max_carat#12333, 'MIN('Carat) AS min_carat#12334]\n   +- SubqueryAlias annotated_data\n      +- View (`annotated_data`, [0.73#12219,VVS1#12220,H+#12221,N#12222,EX4#12223,10500.0#12224,RD#12225,EX7#12226,0.08#12227,0.09#12228,8.0#12229,0.011#12230,0.012#12231,0.013#12232,14383.56#12233,2#12234])\n         +- Relation [0.73#12219,VVS1#12220,H+#12221,N#12222,EX4#12223,10500.0#12224,RD#12225,EX7#12226,0.08#12227,0.09#12228,8.0#12229,0.011#12230,0.012#12231,0.013#12232,14383.56#12233,2#12234] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 39\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Example query\u001b[39;00m\n\u001b[1;32m     33\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124mSELECT Shape, MAX(Carat) as max_carat, MIN(Carat) as min_carat\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124mFROM annotated_data\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124mGROUP BY Shape\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124mORDER BY Shape\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mrun_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 24\u001b[0m, in \u001b[0;36mrun_query\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     21\u001b[0m df\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotated_data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Run the SQL query\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m result_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Show the results\u001b[39;00m\n\u001b[1;32m     27\u001b[0m result_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/de-prj/de-venv/lib/python3.10/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/de-prj/de-venv/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/de-prj/de-venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Shape` cannot be resolved. Did you mean one of the following? [`2`, `8.0`, `EX4`, `EX7`, `H+`].; line 2 pos 7;\n'Sort ['Shape ASC NULLS FIRST], true\n+- 'Aggregate ['Shape], ['Shape, 'MAX('Carat) AS max_carat#12333, 'MIN('Carat) AS min_carat#12334]\n   +- SubqueryAlias annotated_data\n      +- View (`annotated_data`, [0.73#12219,VVS1#12220,H+#12221,N#12222,EX4#12223,10500.0#12224,RD#12225,EX7#12226,0.08#12227,0.09#12228,8.0#12229,0.011#12230,0.012#12231,0.013#12232,14383.56#12233,2#12234])\n         +- Relation [0.73#12219,VVS1#12220,H+#12221,N#12222,EX4#12223,10500.0#12224,RD#12225,EX7#12226,0.08#12227,0.09#12228,8.0#12229,0.011#12230,0.012#12231,0.013#12232,14383.56#12233,2#12234] csv\n"
     ]
    }
   ],
   "source": [
    "def run_query(query: str):\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"HDFSQueryApp\") \\\n",
    "        .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Path to the HDFS file\n",
    "    hdfs_path = \"/user/student/annotated_data\"\n",
    "\n",
    "    # Read the CSV file from HDFS\n",
    "    df = spark.read.option(\"header\", \"true\").csv(hdfs_path)\n",
    "    \n",
    "    # Show first 5 rows to check the column names\n",
    "    df.show(5)\n",
    "    \n",
    "    # Print schema to inspect column names\n",
    "    df.printSchema()\n",
    "\n",
    "    # Register DataFrame as a temporary view\n",
    "    df.createOrReplaceTempView(\"annotated_data\")\n",
    "\n",
    "    # Run the SQL query\n",
    "    result_df = spark.sql(query)\n",
    "\n",
    "    # Show the results\n",
    "    result_df.show()\n",
    "\n",
    "    # Stop Spark session\n",
    "    spark.stop()\n",
    "\n",
    "# Example query\n",
    "query = \"\"\"\n",
    "SELECT Shape, MAX(Carat) as max_carat, MIN(Carat) as min_carat\n",
    "FROM annotated_data\n",
    "GROUP BY Shape\n",
    "ORDER BY Shape\n",
    "\"\"\"\n",
    "run_query(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badd6651-97e1-4f57-b479-9f5a5d7ccaee",
   "metadata": {},
   "source": [
    "## Data Annotation from HBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d8d6b45-0ae6-4ff8-a526-3c7d0db66a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-------+-----+------------+------+-------+-----+--------+-----------+\n",
      "|     row_key|Carat|Clarity|Color|Fluorescence|Polish|  Price|Shape|Symmetry|price_label|\n",
      "+------------+-----+-------+-----+------------+------+-------+-----+--------+-----------+\n",
      "|           0| 0.73|   VVS1|   H+|           N|    EX|10500.0|   RD|      EX|          2|\n",
      "|           1| 0.58|   VVS1|   E+|           N|    EX|10749.0|   RD|      EX|          2|\n",
      "|          10| 0.73|   VVS1|   H+|           N|    EX|10500.0|   RD|      EX|          2|\n",
      "|103079215104| 0.35|   VVS1|    E|           N|    EX| 3866.0|   RD|      EX|          0|\n",
      "|103079215105| 0.31|   VVS1|    E|           N|    EX| 3220.0|   RD|      EX|          0|\n",
      "|103079215106| 0.37|   VVS2|    D|           F|    EX| 3184.0|   RD|      EX|          0|\n",
      "|103079215107|  0.5|   VVS1|   F+|           N|    EX| 7719.0|   RD|      EX|          1|\n",
      "|103079215108|  0.5|   VVS1|   E+|           N|    EX| 8603.0|   RD|      EX|          1|\n",
      "|103079215109| 0.34|   VVS1|    F|           N|    VG| 3407.0|   RD|      EX|          0|\n",
      "|103079215110|  0.5|   VVS2|   G+|           F|    EX| 4971.0|   RD|      EX|          0|\n",
      "+------------+-----+-------+-----+------------+------+-------+-----+--------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to HDFS at /user/student/annotated_data\n",
      "Spark session stopped.\n",
      "HBase connection closed.\n"
     ]
    }
   ],
   "source": [
    "import happybase\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import when, col, lit\n",
    "from pyspark.sql.types import DoubleType, StringType, StructType, StructField\n",
    "\n",
    "class DataAnnotator:\n",
    "    def __init__(self, hbase_table: str, hbase_host: str = 'localhost'):\n",
    "        \"\"\"\n",
    "        Initializes the DataAnnotator by reading data from HBase.\n",
    "\n",
    "        :param hbase_table: The HBase table name\n",
    "        :param hbase_host: The HBase host address\n",
    "        \"\"\"\n",
    "        # Initialize Spark session\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"DataAnnotatorApp\") \\\n",
    "            .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        # Initialize HBase connection\n",
    "        self.connection = happybase.Connection(hbase_host)\n",
    "        self.table = self.connection.table(hbase_table)\n",
    "        \n",
    "        # Read data from HBase and convert it to DataFrame\n",
    "        self.df = self._read_from_hbase()\n",
    "\n",
    "    def _read_from_hbase(self) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Reads data from HBase table and converts it to a DataFrame.\n",
    "\n",
    "        :return: DataFrame with the data from HBase\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Scan the HBase table\n",
    "            rows = self.table.scan()\n",
    "            \n",
    "            # Extract rows into a list of dictionaries\n",
    "            data = []\n",
    "            for key, row in rows:\n",
    "                row_dict = {\n",
    "                    'row_key': key.decode('utf-8'),\n",
    "                    'Carat': row.get(b'cf:Carat', b'').decode('utf-8'),\n",
    "                    'Clarity': row.get(b'cf:Clarity', b'').decode('utf-8'),\n",
    "                    'Color': row.get(b'cf:Color', b'').decode('utf-8'),\n",
    "                    'Fluorescence': row.get(b'cf:Fluorescence', b'').decode('utf-8'),\n",
    "                    'Polish': row.get(b'cf:Polish', b'').decode('utf-8'),\n",
    "                    'Price': row.get(b'cf:Price', b'').decode('utf-8'),\n",
    "                    'Shape': row.get(b'cf:Shape', b'').decode('utf-8'),\n",
    "                    'Symmetry': row.get(b'cf:Symmetry', b'').decode('utf-8')\n",
    "                }\n",
    "                data.append(row_dict)\n",
    "            \n",
    "            # Define schema\n",
    "            schema = StructType([\n",
    "                StructField(\"row_key\", StringType(), True),\n",
    "                StructField(\"Carat\", StringType(), True),\n",
    "                StructField(\"Clarity\", StringType(), True),\n",
    "                StructField(\"Color\", StringType(), True),\n",
    "                StructField(\"Fluorescence\", StringType(), True),\n",
    "                StructField(\"Polish\", StringType(), True),\n",
    "                StructField(\"Price\", StringType(), True),\n",
    "                StructField(\"Shape\", StringType(), True),\n",
    "                StructField(\"Symmetry\", StringType(), True),\n",
    "            ])\n",
    "            \n",
    "            # Convert list of dictionaries to DataFrame\n",
    "            df = self.spark.createDataFrame(data, schema)\n",
    "            \n",
    "            # Convert columns to appropriate data types\n",
    "            df = df.withColumn(\"Carat\", col(\"Carat\").cast(DoubleType())) \\\n",
    "                   .withColumn(\"Price\", col(\"Price\").cast(DoubleType()))\n",
    "            \n",
    "            # Fill null values with defaults\n",
    "            df = df.na.fill({\n",
    "                'Carat': 0.0,\n",
    "                'Price': 0.0,\n",
    "                'Clarity': 'Unknown',\n",
    "                'Color': 'Unknown',\n",
    "                'Fluorescence': 'Unknown',\n",
    "                'Polish': 'Unknown',\n",
    "                'Shape': 'Unknown',\n",
    "                'Symmetry': 'Unknown'\n",
    "            })\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading from HBase: {e}\")\n",
    "            # Use a schema with the same structure to return an empty DataFrame\n",
    "            schema = StructType([\n",
    "                StructField(\"row_key\", StringType(), True),\n",
    "                StructField(\"Carat\", DoubleType(), True),\n",
    "                StructField(\"Clarity\", StringType(), True),\n",
    "                StructField(\"Color\", StringType(), True),\n",
    "                StructField(\"Fluorescence\", StringType(), True),\n",
    "                StructField(\"Polish\", StringType(), True),\n",
    "                StructField(\"Price\", DoubleType(), True),\n",
    "                StructField(\"Shape\", StringType(), True),\n",
    "                StructField(\"Symmetry\", StringType(), True),\n",
    "            ])\n",
    "            return self.spark.createDataFrame(self.spark.sparkContext.emptyRDD(), schema)\n",
    "\n",
    "    def annotate_price(self, high_threshold: float, low_threshold: float):\n",
    "        \"\"\"\n",
    "        Adds the 'price_label' column based on the thresholds.\n",
    "\n",
    "        :param high_threshold: The price threshold for labeling 'High'\n",
    "        :param low_threshold: The price threshold for labeling 'Low'\n",
    "        :return: DataFrame with the new 'price_label' column\n",
    "        \"\"\"\n",
    "        if self.df is not None:\n",
    "            self.df = self.df.withColumn(\n",
    "                'price_label',\n",
    "                when(col('Price') > high_threshold, lit(2))\n",
    "                .when(col('Price') < low_threshold, lit(0))\n",
    "                .otherwise(lit(1))\n",
    "            )\n",
    "        else:\n",
    "            print(\"DataFrame is not initialized.\")\n",
    "        return self.df\n",
    "\n",
    "    def show_data(self, num_rows=10):\n",
    "        \"\"\"\n",
    "        Shows the updated DataFrame with the annotated 'price_label' column.\n",
    "\n",
    "        :param num_rows: Number of rows to display\n",
    "        \"\"\"\n",
    "        if self.df is not None:\n",
    "            self.df.show(num_rows)\n",
    "        else:\n",
    "            print(\"DataFrame is not initialized.\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Stops the Spark session and closes the HBase connection.\n",
    "        \"\"\"\n",
    "        if self.spark:\n",
    "            self.spark.stop()\n",
    "            print(\"Spark session stopped.\")\n",
    "        else:\n",
    "            print(\"Spark session is not initialized.\")\n",
    "        \n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "            print(\"HBase connection closed.\")\n",
    "\n",
    "class HDFSDataSaver:\n",
    "    def __init__(self, output_path: str, file_format: str = 'csv', mode: str = 'overwrite'):\n",
    "        \"\"\"\n",
    "        Initializes the HDFSDataSaver.\n",
    "\n",
    "        :param output_path: Path where the data should be saved\n",
    "        :param file_format: Format of the saved file\n",
    "        :param mode: Save mode\n",
    "        \"\"\"\n",
    "        self.output_path = output_path\n",
    "        self.file_format = file_format\n",
    "        self.mode = mode\n",
    "\n",
    "    def save(self, df: DataFrame):\n",
    "        \"\"\"\n",
    "        Saves the DataFrame to HDFS.\n",
    "\n",
    "        :param df: DataFrame to save\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df.write \\\n",
    "                .format(self.file_format) \\\n",
    "                .mode(self.mode) \\\n",
    "                .save(self.output_path)\n",
    "            print(f\"Data saved to HDFS at {self.output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data to HDFS: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define HBase table name and host\n",
    "    hbase_table = \"processed_data_hbase\"\n",
    "    hbase_host = \"localhost\"\n",
    "\n",
    "    # Initialize DataAnnotator with HBase table\n",
    "    annotator = DataAnnotator(hbase_table=hbase_table, hbase_host=hbase_host)\n",
    "    \n",
    "    # Define thresholds for annotation\n",
    "    high_threshold = 10000.0\n",
    "    low_threshold = 5000.0\n",
    "    \n",
    "    # Annotate data\n",
    "    df_annotated = annotator.annotate_price(high_threshold, low_threshold)\n",
    "    annotator.show_data()\n",
    "\n",
    "    # Save to HDFS\n",
    "    saver = HDFSDataSaver(output_path=\"/user/student/annotated_data\", file_format='csv', mode='overwrite')\n",
    "    saver.save(df_annotated)\n",
    "\n",
    "    # Close connections\n",
    "    annotator.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896269b9-502f-4198-8de2-38ce523dbb99",
   "metadata": {},
   "source": [
    "### HDFS Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e21ce4a-9ec5-42e6-b222-a378acf79a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/10 22:07:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Data\n",
      "+----+----+---+---+---+-------+---+---+----+----+---+-----+-----+-----+--------+---+\n",
      "|0.73|VVS1| H+|  N|EX4|10500.0| RD|EX7|0.08|0.09|8.0|0.011|0.012|0.013|14383.56|  2|\n",
      "+----+----+---+---+---+-------+---+---+----+----+---+-----+-----+-----+--------+---+\n",
      "|0.58|VVS1| E+|  N| EX|10749.0| RD| EX| 0.0| 0.0|3.0|  0.0|  0.0|  0.0|18532.76|  2|\n",
      "|0.58|VVS1| E+|  N| EX|10749.0| RD| EX| 0.0| 0.0|3.0|  0.0|  0.0|  0.0|18532.76|  2|\n",
      "|0.58|VVS1| E+|  N| EX|10749.0| RD| EX| 0.0| 0.0|3.0|  0.0|  0.0|  0.0|18532.76|  2|\n",
      "|0.57|VVS1| E+|  N| EX|10250.0| RD| EX| 0.0| 0.0|3.0|  0.0|  0.0|  0.0|17982.46|  2|\n",
      "|0.73|VVS1| H+|  N| EX|10500.0| RD| EX| 0.0| 0.0|8.0|  0.0|  0.0|  0.0|14383.56|  2|\n",
      "+----+----+---+---+---+-------+---+---+----+----+---+-----+-----+-----+--------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/10 22:07:55 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 0.73, VVS1, H+, N, EX, 10500.0, RD, EX, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 14383.56, 2\n",
      " Schema: 0.73, VVS1, H+, N, EX4, 10500.0, RD, EX7, 0.08, 0.09, 8.0, 0.011, 0.012, 0.013, 14383.56, 2\n",
      "Expected: EX4 but found: EX\n",
      "CSV file: hdfs://localhost:9000/user/student/annotated_data/part-00000-8c3357fc-ab3e-49f1-9c4f-0b301da0b087-c000.csv\n"
     ]
    }
   ],
   "source": [
    "#check saved annotated_data dataset \n",
    "class HDFSDataReader:\n",
    "    def __init__(self, hdfs_path: str):\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"ReadHDFS\") \\\n",
    "            .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "            .getOrCreate()\n",
    "        self.hdfs_path = hdfs_path\n",
    "\n",
    "    def read_data(self) -> DataFrame:\n",
    "        self.df = self.spark.read.json(self.hdfs_path)\n",
    "        return self.df\n",
    "\n",
    "    def read_csv(self, path: str) -> DataFrame:\n",
    "        self.df = self.spark.read.csv(path, header=True, inferSchema=True)\n",
    "        return self.df\n",
    "\n",
    "    def show_data(self, num_rows=5):\n",
    "        if hasattr(self, 'df'):\n",
    "            print(\"Preprocessed Data\")\n",
    "            self.df.show(num_rows)\n",
    "        else:\n",
    "            print(\"DataFrame not loaded yet. Call read_data() or read_csv() first.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hdfs_reader = HDFSDataReader(\"/user/student/annotated_data\")\n",
    "\n",
    "    # Reading the saved CSV file\n",
    "    df_saved = hdfs_reader.read_csv(\"/user/student/annotated_data\")\n",
    "\n",
    "    hdfs_reader.show_data(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "803bc387-be3d-4a2c-b99f-305da27115dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Carat: double (nullable = true)\n",
      " |-- Clarity: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Fluorescence: string (nullable = true)\n",
      " |-- Polish: string (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Shape: string (nullable = true)\n",
      " |-- Symmetry: string (nullable = true)\n",
      " |-- Shape_encoded: double (nullable = true)\n",
      " |-- Clarity_encoded: double (nullable = true)\n",
      " |-- Color_encoded: double (nullable = true)\n",
      " |-- Polish_encoded: double (nullable = true)\n",
      " |-- Symmetry_encoded: double (nullable = true)\n",
      " |-- Fluorescence_encoded: double (nullable = true)\n",
      " |-- Price per Carat: double (nullable = true)\n",
      " |-- price_label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_saved.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c950f6d7-7b4d-4b58-8f0b-71778dad30ca",
   "metadata": {},
   "source": [
    "## Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f2b7c0-ff45-46ad-8d1e-86580360fd76",
   "metadata": {},
   "source": [
    "## 2.4 Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a26b55a-1816-4b90-996e-704da716349a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/10 21:00:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/09/10 21:00:42 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Data\n",
      "+-----+-------+-----+------------+------+-------+-----+--------+-------------+---------------+-------------+--------------+----------------+--------------------+---------------+-----------+----+\n",
      "|Carat|Clarity|Color|Fluorescence|Polish|  Price|Shape|Symmetry|Shape_encoded|Clarity_encoded|Color_encoded|Polish_encoded|Symmetry_encoded|Fluorescence_encoded|Price per Carat|price_label| low|\n",
      "+-----+-------+-----+------------+------+-------+-----+--------+-------------+---------------+-------------+--------------+----------------+--------------------+---------------+-----------+----+\n",
      "| 0.73|   VVS1|   H+|           N|    EX|10500.0|   RD|      EX|          0.0|            0.0|          8.0|           0.0|             0.0|                 0.0|       14383.56|          2|high|\n",
      "| 0.58|   VVS1|   E+|           N|    EX|10749.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       18532.76|          2|high|\n",
      "| 0.58|   VVS1|   E+|           N|    EX|10749.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       18532.76|          2|high|\n",
      "| 0.58|   VVS1|   E+|           N|    EX|10749.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       18532.76|          2|high|\n",
      "| 0.57|   VVS1|   E+|           N|    EX|10250.0|   RD|      EX|          0.0|            0.0|          3.0|           0.0|             0.0|                 0.0|       17982.46|          2|high|\n",
      "+-----+-------+-----+------------+------+-------+-----+--------+-------------+---------------+-------------+--------------+----------------+--------------------+---------------+-----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/10 21:00:43 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Count: 1503\n",
      "Test Data Count: 412\n",
      "Data saved to /user/student/train_data as csv\n",
      "Data saved to /user/student/test_data as csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "class TrainTestSplitter:\n",
    "    def __init__(self, dataframe: DataFrame, train_ratio: float = 0.8, seed: int = 42):\n",
    "        self.df = dataframe\n",
    "        self.train_ratio = train_ratio\n",
    "        self.seed = seed\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"TrainTestSplitter\") \\\n",
    "            .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "    def split(self):\n",
    "        self.train_df, self.test_df = self.df.randomSplit([self.train_ratio, 1 - self.train_ratio], seed=self.seed)\n",
    "        print(f\"Training Data Count: {self.train_df.count()}\")\n",
    "        print(f\"Test Data Count: {self.test_df.count()}\")\n",
    "        return self.train_df, self.test_df\n",
    "\n",
    "    def save_as_csv(self, train_path: str, test_path: str):\n",
    "        if hasattr(self, 'train_df') and hasattr(self, 'test_df'):\n",
    "            self.train_df.write.mode('overwrite').csv(train_path)\n",
    "            self.test_df.write.mode('overwrite').csv(test_path)\n",
    "            print(f\"Training data saved to {train_path}\")\n",
    "            print(f\"Testing data saved to {test_path}\")\n",
    "        else:\n",
    "            print(\"DataFrames not split yet. Call split() first.\")\n",
    "\n",
    "    def close(self):\n",
    "        self.spark.stop()\n",
    "        print(\"Spark session stopped.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hdfs_path = \"/user/student/streamed_data\"  # Set file path to read from\n",
    "    hdfs_reader = HDFSDataReader(hdfs_path)\n",
    "    df = hdfs_reader.read_csv(hdfs_path)\n",
    "\n",
    "    if df.count() > 0:\n",
    "        hdfs_reader.show_data(5)\n",
    "        \n",
    "        # Split the data into train and test datasets \n",
    "        splitter = TrainTestSplitter(df, train_ratio=0.8, seed=1234)\n",
    "        train_df, test_df = splitter.split()  # No argument needed here\n",
    "        \n",
    "        # Save the train and test datasets\n",
    "        train_saver = HDFSDataSaver(output_path=\"/user/student/train_data\", file_format='csv', mode='overwrite')\n",
    "        test_saver = HDFSDataSaver(output_path=\"/user/student/test_data\", file_format='csv', mode='overwrite')\n",
    "\n",
    "        # Save the datasets\n",
    "        train_saver.save(train_df)\n",
    "        test_saver.save(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad98cc66-e6cd-4045-b033-32097f51eddb",
   "metadata": {},
   "source": [
    "## Store: Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e674b53-8326-4de0-8033-33ca23c515f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_141788/2376923209.py:22: DeprecationWarning: Redis.hmset() is deprecated. Use Redis.hset() instead.\n",
      "  self.r.hmset(redis_key, sanitized_row)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to Redis with key prefix: train_df\n",
      "Data successfully saved to Redis with key prefix: test_df\n"
     ]
    }
   ],
   "source": [
    "#store train_df and test_df into redis\n",
    "import redis\n",
    "from pyspark.sql import Row\n",
    "\n",
    "class RedisDataSaver:\n",
    "    def __init__(self, host='localhost', port=6379, db=0):\n",
    "        # Initialize Redis connection\n",
    "        self.r = redis.StrictRedis(host=host, port=port, db=db, decode_responses=True)\n",
    "\n",
    "    def save_to_redis(self, dataframe: DataFrame, key_prefix: str):\n",
    "        try:\n",
    "            # Convert DataFrame rows to dictionary format for Redis storage\n",
    "            for idx, row in enumerate(dataframe.collect()):\n",
    "                # Convert the row to a dictionary, handling NoneType values\n",
    "                row_dict = row.asDict()\n",
    "                sanitized_row = {k: (str(v) if v is not None else \"null\") for k, v in row_dict.items()}\n",
    "\n",
    "                # Create a Redis key for each row\n",
    "                redis_key = f\"{key_prefix}:{idx}\"\n",
    "\n",
    "                # Save the dictionary to Redis as a hash\n",
    "                self.r.hmset(redis_key, sanitized_row)\n",
    "\n",
    "            print(f\"Data successfully saved to Redis with key prefix: {key_prefix}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data to Redis: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    redis_saver = RedisDataSaver()\n",
    "    \n",
    "    # Save the processed DataFrame to Redis\n",
    "    redis_saver.save_to_redis(train_df, key_prefix=\"train_df\")\n",
    "    redis_saver.save_to_redis(train_df, key_prefix=\"test_df\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d0edbb-14a5-4cdc-b065-ded90a5c64c3",
   "metadata": {},
   "source": [
    "### Read from Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9644c068-ab53-4adf-92f9-4cfbf0b266ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Carat': '0.3', 'Clarity': 'VVS1', 'Color': 'F+', 'Fluorescence': 'N', 'Polish': 'EX', 'Price': '2938.0', 'Shape': 'RD', 'Symmetry': 'EX', 'Shape_encoded': '0.0', 'Clarity_encoded': '0.0', 'Color_encoded': '7.0', 'Polish_encoded': '0.0', 'Symmetry_encoded': '0.0', 'Fluorescence_encoded': '0.0', 'Price per Carat': '9793.33', 'price_label': '0', 'low': 'low'}\n",
      "{'Carat': '0.3', 'Clarity': 'VVS2', 'Color': 'G+', 'Fluorescence': 'F', 'Polish': 'EX', 'Price': '2028.0', 'Shape': 'RD', 'Symmetry': 'EX', 'Shape_encoded': '0.0', 'Clarity_encoded': '1.0', 'Color_encoded': '6.0', 'Polish_encoded': '0.0', 'Symmetry_encoded': '0.0', 'Fluorescence_encoded': '1.0', 'Price per Carat': '6760.0', 'price_label': '0', 'low': 'low'}\n",
      "{'Carat': '0.3', 'Clarity': 'VVS2', 'Color': 'H+', 'Fluorescence': 'N', 'Polish': 'EX', 'Price': '2103.0', 'Shape': 'RD', 'Symmetry': 'EX', 'Shape_encoded': '0.0', 'Clarity_encoded': '1.0', 'Color_encoded': '8.0', 'Polish_encoded': '0.0', 'Symmetry_encoded': '0.0', 'Fluorescence_encoded': '0.0', 'Price per Carat': '7010.0', 'price_label': '0', 'low': 'low'}\n",
      "{'Carat': '0.3', 'Clarity': 'VVS2', 'Color': 'H+', 'Fluorescence': 'N', 'Polish': 'VG', 'Price': '2142.0', 'Shape': 'RD', 'Symmetry': 'VG', 'Shape_encoded': '0.0', 'Clarity_encoded': '1.0', 'Color_encoded': '8.0', 'Polish_encoded': '1.0', 'Symmetry_encoded': '1.0', 'Fluorescence_encoded': '0.0', 'Price per Carat': '7140.0', 'price_label': '0', 'low': 'low'}\n",
      "{'Carat': '0.31', 'Clarity': 'VVS1', 'Color': 'D', 'Fluorescence': 'N', 'Polish': 'EX', 'Price': '3290.0', 'Shape': 'RD', 'Symmetry': 'EX', 'Shape_encoded': '0.0', 'Clarity_encoded': '0.0', 'Color_encoded': '0.0', 'Polish_encoded': '0.0', 'Symmetry_encoded': '0.0', 'Fluorescence_encoded': '0.0', 'Price per Carat': '10612.9', 'price_label': '0', 'low': 'low'}\n",
      "{'Carat': '0.31', 'Clarity': 'VVS1', 'Color': 'D', 'Fluorescence': 'N', 'Polish': 'EX', 'Price': '3365.0', 'Shape': 'RD', 'Symmetry': 'EX', 'Shape_encoded': '0.0', 'Clarity_encoded': '0.0', 'Color_encoded': '0.0', 'Polish_encoded': '0.0', 'Symmetry_encoded': '0.0', 'Fluorescence_encoded': '0.0', 'Price per Carat': '10854.84', 'price_label': '0', 'low': 'low'}\n",
      "{'Carat': '0.31', 'Clarity': 'VVS1', 'Color': 'D', 'Fluorescence': 'N', 'Polish': 'EX', 'Price': '3365.0', 'Shape': 'RD', 'Symmetry': 'EX', 'Shape_encoded': '0.0', 'Clarity_encoded': '0.0', 'Color_encoded': '0.0', 'Polish_encoded': '0.0', 'Symmetry_encoded': '0.0', 'Fluorescence_encoded': '0.0', 'Price per Carat': '10854.84', 'price_label': '0', 'low': 'low'}\n",
      "{'Carat': '0.31', 'Clarity': 'VVS1', 'Color': 'D', 'Fluorescence': 'N', 'Polish': 'EX', 'Price': '3413.0', 'Shape': 'RD', 'Symmetry': 'EX', 'Shape_encoded': '0.0', 'Clarity_encoded': '0.0', 'Color_encoded': '0.0', 'Polish_encoded': '0.0', 'Symmetry_encoded': '0.0', 'Fluorescence_encoded': '0.0', 'Price per Carat': '11009.68', 'price_label': '0', 'low': 'low'}\n",
      "{'Carat': '0.31', 'Clarity': 'VVS1', 'Color': 'D', 'Fluorescence': 'N', 'Polish': 'EX', 'Price': '3413.0', 'Shape': 'RD', 'Symmetry': 'EX', 'Shape_encoded': '0.0', 'Clarity_encoded': '0.0', 'Color_encoded': '0.0', 'Polish_encoded': '0.0', 'Symmetry_encoded': '0.0', 'Fluorescence_encoded': '0.0', 'Price per Carat': '11009.68', 'price_label': '0', 'low': 'low'}\n",
      "{'Carat': '0.31', 'Clarity': 'VVS1', 'Color': 'D', 'Fluorescence': 'N', 'Polish': 'EX', 'Price': '3465.0', 'Shape': 'RD', 'Symmetry': 'EX', 'Shape_encoded': '0.0', 'Clarity_encoded': '0.0', 'Color_encoded': '0.0', 'Polish_encoded': '0.0', 'Symmetry_encoded': '0.0', 'Fluorescence_encoded': '0.0', 'Price per Carat': '11177.42', 'price_label': '0', 'low': 'low'}\n"
     ]
    }
   ],
   "source": [
    "#read train_df from redis\n",
    "import redis\n",
    "\n",
    "class RedisDataReader:\n",
    "    def __init__(self, host='localhost', port=6379, db=0):\n",
    "        self.redis_client = redis.StrictRedis(host=host, port=port, db=db)\n",
    "\n",
    "    def read_from_redis(self, key_prefix: str, count: int):\n",
    "        data = []\n",
    "        \n",
    "        for i in range(count):\n",
    "            redis_key = f\"{key_prefix}:{i}\"\n",
    "            row = self.redis_client.hgetall(redis_key)\n",
    "            \n",
    "            row_decoded = {k.decode('utf-8'): v.decode('utf-8') for k, v in row.items()}\n",
    "            \n",
    "            if row_decoded:  # If the row exists\n",
    "                data.append(row_decoded)\n",
    "            else:\n",
    "                print(f\"No data found for key: {redis_key}\")\n",
    "        \n",
    "        return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    redis_reader = RedisDataReader()\n",
    "    redis_data = redis_reader.read_from_redis(key_prefix=\"train_df\", count=10)  # Fetch the first 10 rows\n",
    "\n",
    "    for row in redis_data:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8daf0a6-0277-40d4-ba57-91d33a72bf45",
   "metadata": {},
   "source": [
    "### Convert Redis to Spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afe42ac6-c2c6-48c9-824f-1601d9139f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/10 21:15:01 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+------------+------+-------+-----+--------+-------------+---------------+-------------+--------------+----------------+--------------------+---------------+-----------+----+\n",
      "|Carat|Clarity|Color|Fluorescence|Polish|  Price|Shape|Symmetry|Shape_encoded|Clarity_encoded|Color_encoded|Polish_encoded|Symmetry_encoded|Fluorescence_encoded|Price per Carat|price_label| low|\n",
      "+-----+-------+-----+------------+------+-------+-----+--------+-------------+---------------+-------------+--------------+----------------+--------------------+---------------+-----------+----+\n",
      "| 0.62|   VVS1|    D|           M|    EX|13898.0|   RD|      EX|          0.0|            0.0|          0.0|           0.0|             0.0|                 2.0|       22416.13|          2|high|\n",
      "| 0.32|   VVS2|    D|           M|    EX| 2428.0|   RD|      EX|          0.0|            1.0|          0.0|           0.0|             0.0|                 2.0|         7587.5|          0| low|\n",
      "|  0.8|   VVS1|    E|           M|    EX|15687.0|   RD|      EX|          0.0|            0.0|          1.0|           0.0|             0.0|                 2.0|       19608.75|          2|high|\n",
      "|  0.8|   VVS1|    E|           M|    EX|15687.0|   RD|      EX|          0.0|            0.0|          1.0|           0.0|             0.0|                 2.0|       19608.75|          2|high|\n",
      "|  0.4|   VVS2|    D|           N|    EX| 4090.0|   RD|      EX|          0.0|            1.0|          0.0|           0.0|             0.0|                 0.0|        10225.0|          0| low|\n",
      "+-----+-------+-----+------------+------+-------+-----+--------+-------------+---------------+-------------+--------------+----------------+--------------------+---------------+-----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#retrieve from redis\n",
    "import redis\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, IntegerType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "r = redis.StrictRedis(host='localhost', port=6379, db=0, decode_responses=True)\n",
    "keys = r.keys('train_df:*')\n",
    "\n",
    "data = []\n",
    "for key in keys:\n",
    "    record = r.hgetall(key)\n",
    "    data.append(record)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Redis to Spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#convert to pyspark dataframe\n",
    "schema = StructType([\n",
    "    StructField(\"Carat\", DoubleType(), True),\n",
    "    StructField(\"Clarity\", StringType(), True),\n",
    "    StructField(\"Color\", StringType(), True),\n",
    "    StructField(\"Fluorescence\", StringType(), True),\n",
    "    StructField(\"Polish\", StringType(), True),\n",
    "    StructField(\"Price\", DoubleType(), True),\n",
    "    StructField(\"Shape\", StringType(), True),\n",
    "    StructField(\"Symmetry\", StringType(), True),\n",
    "    StructField(\"Shape_encoded\", DoubleType(), True),\n",
    "    StructField(\"Clarity_encoded\", DoubleType(), True),\n",
    "    StructField(\"Color_encoded\", DoubleType(), True),\n",
    "    StructField(\"Polish_encoded\", DoubleType(), True),\n",
    "    StructField(\"Symmetry_encoded\", DoubleType(), True),\n",
    "    StructField(\"Fluorescence_encoded\", DoubleType(), True),\n",
    "    StructField(\"Price per Carat\", DoubleType(), True),\n",
    "    StructField(\"price_label\", IntegerType(), True),\n",
    "    StructField(\"low\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Convert data to rows and handle null values\n",
    "def convert_value(k, v):\n",
    "    if v is None or v == 'null':\n",
    "        return None \n",
    "    if k.endswith('_encoded') or k in ['Carat', 'Price', 'Price per Carat']:\n",
    "        return float(v) if v.replace('.', '', 1).isdigit() else None\n",
    "    if k == 'price_label':\n",
    "        return int(v) if v.isdigit() else None\n",
    "    return v \n",
    "\n",
    "rows = [Row(**{k: convert_value(k, v) for k, v in record.items()}) for record in data]\n",
    "\n",
    "train_df = spark.createDataFrame(rows, schema)\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34361665-34c4-4536-81f1-b3e67b6fe0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+------------+------+------+-----+--------+-------------+---------------+-------------+--------------+----------------+--------------------+---------------+-----------+---+\n",
      "|Carat|Clarity|Color|Fluorescence|Polish| Price|Shape|Symmetry|Shape_encoded|Clarity_encoded|Color_encoded|Polish_encoded|Symmetry_encoded|Fluorescence_encoded|Price per Carat|price_label|low|\n",
      "+-----+-------+-----+------------+------+------+-----+--------+-------------+---------------+-------------+--------------+----------------+--------------------+---------------+-----------+---+\n",
      "| 0.31|     IF|    F|           N|    EX|3172.0|   RD|      EX|          0.0|            2.0|          2.0|           0.0|             0.0|                 0.0|       10232.26|          0|low|\n",
      "|  0.3|   VVS2|   G+|           F|    EX|2028.0|   RD|      EX|          0.0|            1.0|          6.0|           0.0|             0.0|                 1.0|         6760.0|          0|low|\n",
      "| 0.35|     IF|    E|           N|    EX|4117.0|   RD|      EX|          0.0|            2.0|          1.0|           0.0|             0.0|                 0.0|       11762.86|          0|low|\n",
      "| 0.43|   VVS1|   E+|           N|    EX|4687.0|   RD|      VG|          0.0|            0.0|          3.0|           0.0|             1.0|                 0.0|        10900.0|          0|low|\n",
      "| 0.33|     IF|    D|           N|    EX|4194.0|   RD|      EX|          0.0|            2.0|          0.0|           0.0|             0.0|                 0.0|       12709.09|          0|low|\n",
      "+-----+-------+-----+------------+------+------+-----+--------+-------------+---------------+-------------+--------------+----------------+--------------------+---------------+-----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#retrieve from redis\n",
    "import redis\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, IntegerType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "r = redis.StrictRedis(host='localhost', port=6379, db=0, decode_responses=True)\n",
    "keys = r.keys('test_df:*')\n",
    "\n",
    "data = []\n",
    "for key in keys:\n",
    "    record = r.hgetall(key)\n",
    "    data.append(record)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Redis to Spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#convert to pyspark dataframe\n",
    "schema = StructType([\n",
    "    StructField(\"Carat\", DoubleType(), True),\n",
    "    StructField(\"Clarity\", StringType(), True),\n",
    "    StructField(\"Color\", StringType(), True),\n",
    "    StructField(\"Fluorescence\", StringType(), True),\n",
    "    StructField(\"Polish\", StringType(), True),\n",
    "    StructField(\"Price\", DoubleType(), True),\n",
    "    StructField(\"Shape\", StringType(), True),\n",
    "    StructField(\"Symmetry\", StringType(), True),\n",
    "    StructField(\"Shape_encoded\", DoubleType(), True),\n",
    "    StructField(\"Clarity_encoded\", DoubleType(), True),\n",
    "    StructField(\"Color_encoded\", DoubleType(), True),\n",
    "    StructField(\"Polish_encoded\", DoubleType(), True),\n",
    "    StructField(\"Symmetry_encoded\", DoubleType(), True),\n",
    "    StructField(\"Fluorescence_encoded\", DoubleType(), True),\n",
    "    StructField(\"Price per Carat\", DoubleType(), True),\n",
    "    StructField(\"price_label\", IntegerType(), True),\n",
    "    StructField(\"low\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Convert data to rows and handle null values\n",
    "def convert_value(k, v):\n",
    "    if v is None or v == 'null':\n",
    "        return None \n",
    "    if k.endswith('_encoded') or k in ['Carat', 'Price', 'Price per Carat']:\n",
    "        return float(v) if v.replace('.', '', 1).isdigit() else None\n",
    "    if k == 'price_label':\n",
    "        return int(v) if v.isdigit() else None\n",
    "    return v \n",
    "\n",
    "rows = [Row(**{k: convert_value(k, v) for k, v in record.items()}) for record in data]\n",
    "\n",
    "test_df = spark.createDataFrame(rows, schema)\n",
    "test_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612cdad9-241c-4dea-98f3-0fb9500d349c",
   "metadata": {},
   "source": [
    "### Redis Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad4a504c-0ca7-4930-9241-400817d462ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|price_label|count|\n",
      "+-----------+-----+\n",
      "|          1|  447|\n",
      "|          2|  267|\n",
      "|          0|  789|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 1: Identify the number of diamonds for each price category\n",
    "grouped_df = df.groupBy(\"price_label\").count()\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99ed0fbd-da36-4572-a9dd-e80682f4bc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|Carat|        avg(Price)|\n",
      "+-----+------------------+\n",
      "| NULL|              NULL|\n",
      "|  0.3| 2858.780104712042|\n",
      "| 0.31|       3280.390625|\n",
      "| 0.32|          3153.675|\n",
      "| 0.33|3682.8823529411766|\n",
      "| 0.34|3608.2162162162163|\n",
      "| 0.35|           3916.72|\n",
      "| 0.36| 3929.032258064516|\n",
      "| 0.37|            3759.0|\n",
      "| 0.38|4040.6470588235293|\n",
      "|  0.4| 4200.068627450981|\n",
      "| 0.41|            4492.7|\n",
      "| 0.42|            4816.0|\n",
      "| 0.43| 4542.951219512195|\n",
      "| 0.44| 4970.357142857143|\n",
      "| 0.45| 4115.976744186047|\n",
      "| 0.46|            5234.4|\n",
      "| 0.47|            3840.0|\n",
      "|  0.5|  6992.25974025974|\n",
      "| 0.51|           7216.34|\n",
      "+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 2: Average price of each carat\n",
    "df.groupBy(\"Carat\").agg({\"Price\": \"avg\"}).orderBy(\"Carat\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a98cec22-6e17-48de-bf9f-929cd6523708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between Price and Carat: 0.8856350156433647\n",
      "Correlation between Price and Price per Carat: 0.9473245649727444\n",
      "Correlation between Price and Shape_encoded: -0.08548364802734425\n",
      "Correlation between Price and Clarity_encoded: -0.1364167124277153\n",
      "Correlation between Price and Color_encoded: -0.15161489094051314\n",
      "Correlation between Price and Polish_encoded: -0.13301420391677396\n",
      "Correlation between Price and Symmetry_encoded: -0.17546699084461193\n",
      "Correlation between Price and Fluorescence_encoded: 0.03088897919073895\n"
     ]
    }
   ],
   "source": [
    "# Query 3: Correlation of feature columns with Price\n",
    "numerical_columns = [\"Carat\", \"Price per Carat\", \"Shape_encoded\", \"Clarity_encoded\", \"Color_encoded\", \n",
    "                     \"Polish_encoded\", \"Symmetry_encoded\", \"Fluorescence_encoded\"]\n",
    "\n",
    "for col in numerical_columns:\n",
    "    correlation = df.stat.corr(\"Price\", col)\n",
    "    print(f\"Correlation between Price and {col}: {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cb1e6e-5b17-4ca3-a85a-42be629efa52",
   "metadata": {},
   "source": [
    "## Read from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffffdc60-5c3a-4a64-b1fd-eaa120578a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check saved train_data dataset\n",
    "class HDFSDataReader:\n",
    "    def __init__(self, hdfs_path: str):\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"ReadHDFS\") \\\n",
    "            .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "            .getOrCreate()\n",
    "        self.hdfs_path = hdfs_path\n",
    "\n",
    "    def read_data(self) -> DataFrame:\n",
    "        self.df = self.spark.read.json(self.hdfs_path)\n",
    "        return self.df\n",
    "    \n",
    "    def read_csv(self, path: str) -> DataFrame:\n",
    "        self.df = self.spark.read.csv(path, header=True, inferSchema=True)\n",
    "        return self.df\n",
    "\n",
    "    def show_data(self, num_rows=5):\n",
    "        if hasattr(self, 'df'):\n",
    "            self.df.show(num_rows)\n",
    "        else:\n",
    "            print(\"DataFrame not loaded yet. Call read_data() or read_csv() first.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hdfs_reader = HDFSDataReader(\"/user/student/train_data\")\n",
    "    df_saved = hdfs_reader.read_csv(\"/user/student/train_data\")\n",
    "    hdfs_reader.show_data(5)\n",
    "    \n",
    "    print(f\"Train data count: {df_saved.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059c116-319c-4254-a22e-0cf020162c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saved.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d321857-52fa-445f-bdb0-f3b08b42ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check saved test_data dataset\n",
    "class HDFSDataReader:\n",
    "    def __init__(self, hdfs_path: str):\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"ReadHDFS\") \\\n",
    "            .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "            .getOrCreate()\n",
    "        self.hdfs_path = hdfs_path\n",
    "\n",
    "    def read_data(self) -> DataFrame:\n",
    "        self.df = self.spark.read.json(self.hdfs_path)\n",
    "        return self.df\n",
    "    \n",
    "    # New method to read CSV\n",
    "    def read_csv(self, path: str) -> DataFrame:\n",
    "        self.df = self.spark.read.csv(path, header=True, inferSchema=True)\n",
    "        return self.df\n",
    "\n",
    "    def show_data(self, num_rows=5):\n",
    "        if hasattr(self, 'df'):\n",
    "            print(\"Preprocessed Data\")\n",
    "            self.df.show(num_rows)\n",
    "        else:\n",
    "            print(\"DataFrame not loaded yet. Call read_data() or read_csv() first.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hdfs_reader = HDFSDataReader(\"/user/student/test_data\")\n",
    "    df_saved = hdfs_reader.read_csv(\"/user/student/test_data\")\n",
    "    hdfs_reader.show_data(5)\n",
    "    \n",
    "    print(f\"Test data count: {df_saved.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd91fe5-82be-4f6d-b9a1-35e9495e2b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saved.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80bc7f6-099a-46e3-ae00-c88d0865653d",
   "metadata": {},
   "source": [
    "### 2.4(a) Random Forest with HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5a17733-cf95-41f9-be5b-5da46313e179",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 71\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark session stopped.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Instantiate the RandomForestModeler\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     rf_modeler \u001b[38;5;241m=\u001b[39m RandomForestModeler(\u001b[43mtrain_path\u001b[49m, test_path)\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# Specify features and label columns\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     feature_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCarat\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClarity_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mColor_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPolish_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymmetry_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFluorescence_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m] \n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_path' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "class RandomForestModeler:\n",
    "    def __init__(self, train_path: str, test_path: str):\n",
    "        self.spark = SparkSession.builder.appName(\"RandomForestModeler\").getOrCreate()\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.train_df = self.load_data(self.train_path)\n",
    "        self.test_df = self.load_data(self.test_path)\n",
    "\n",
    "    def load_data(self, path: str):\n",
    "        return self.spark.read.csv(path, header=True, inferSchema=True)\n",
    "\n",
    "    def prepare_data(self, df, feature_cols, label_col):\n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "        df_prepared = assembler.transform(df).select(\"features\", label_col)\n",
    "        return df_prepared\n",
    "\n",
    "    def train_random_forest(self, feature_cols, label_col, num_trees=10):\n",
    "        # Prepare training data\n",
    "        train_data = self.prepare_data(self.train_df, feature_cols, label_col)\n",
    "\n",
    "        # Initialize the Random Forest Classifier\n",
    "        rf = RandomForestClassifier(featuresCol=\"features\", labelCol=label_col, numTrees=num_trees)\n",
    "        \n",
    "        # Train the model\n",
    "        self.rf_model = rf.fit(train_data)\n",
    "        print(\"Random Forest model trained.\")\n",
    "\n",
    "    def evaluate_model(self, feature_cols, label_col):\n",
    "        # Prepare test data\n",
    "        test_data = self.prepare_data(self.test_df, feature_cols, label_col)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = self.rf_model.transform(test_data)\n",
    "\n",
    "        # Initialize evaluators for different metrics\n",
    "        accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "        f1_evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"f1\")\n",
    "        precision_evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "        recall_evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "\n",
    "        # Compute metrics\n",
    "        accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "        f1_score = f1_evaluator.evaluate(predictions)\n",
    "        precision = precision_evaluator.evaluate(predictions)\n",
    "        recall = recall_evaluator.evaluate(predictions)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "        print(f\"F1 Score: {f1_score:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall\n",
    "        }\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Stops the Spark session.\n",
    "        \"\"\"\n",
    "        self.spark.stop()\n",
    "        print(\"Spark session stopped.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_path = \"/user/student/train_data\"\n",
    "    test_path = \"/user/student/test_data\"\n",
    "\n",
    "    # Instantiate the RandomForestModeler\n",
    "    rf_modeler = RandomForestModeler(train_path, test_path)\n",
    "\n",
    "    # Specify features and label columns\n",
    "    feature_columns = ['Carat','Shape_encoded', 'Clarity_encoded', 'Color_encoded', 'Polish_encoded','Symmetry_encoded','Fluorescence_encoded'] \n",
    "    label_column = 'price_label'  \n",
    "\n",
    "    # Train the Random Forest model\n",
    "    rf_modeler.train_random_forest(feature_cols=feature_columns, label_col=label_column, num_trees=20)\n",
    "    \n",
    "    #Metrics\n",
    "    metrics = rf_modeler.evaluate_model(feature_cols=feature_columns, label_col=label_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf541c9-fedf-4abf-8ffa-78dc77d64aaa",
   "metadata": {},
   "source": [
    "### Random Forest with Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76a255b3-acbb-4f13-8a10-e6fa9d75df5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/10 21:31:40 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "24/09/10 21:31:40 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest model trained.\n",
      "Accuracy: 92.55%\n",
      "F1 Score: 0.9248\n",
      "Precision: 0.9252\n",
      "Recall: 0.9255\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "class RandomForestModeler:\n",
    "    def __init__(self, train_df, test_df):\n",
    "        self.spark = SparkSession.builder.appName(\"RandomForestModeler\").getOrCreate()\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "    def prepare_data(self, df, feature_cols, label_col):\n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "        df_prepared = assembler.transform(df).select(\"features\", label_col)\n",
    "        return df_prepared\n",
    "\n",
    "    def train_random_forest(self, feature_cols, label_col, num_trees=10):\n",
    "        # Prepare training data\n",
    "        train_data = self.prepare_data(self.train_df, feature_cols, label_col)\n",
    "\n",
    "        # Initialize the Random Forest Classifier\n",
    "        rf = RandomForestClassifier(featuresCol=\"features\", labelCol=label_col, numTrees=num_trees)\n",
    "        \n",
    "        # Train the model\n",
    "        self.rf_model = rf.fit(train_data)\n",
    "        print(\"Random Forest model trained.\")\n",
    "\n",
    "    def evaluate_model(self, feature_cols, label_col):\n",
    "        # Prepare test data\n",
    "        test_data = self.prepare_data(self.test_df, feature_cols, label_col)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = self.rf_model.transform(test_data)\n",
    "\n",
    "        # Initialize evaluators for different metrics\n",
    "        accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "        f1_evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"f1\")\n",
    "        precision_evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "        recall_evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "\n",
    "        # Compute metrics\n",
    "        accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "        f1_score = f1_evaluator.evaluate(predictions)\n",
    "        precision = precision_evaluator.evaluate(predictions)\n",
    "        recall = recall_evaluator.evaluate(predictions)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "        print(f\"F1 Score: {f1_score:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall\n",
    "        }\n",
    "\n",
    "    def close(self):\n",
    "        self.spark.stop()\n",
    "        print(\"Spark session stopped.\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"MainExecution\").getOrCreate()\n",
    "    \n",
    "    # Specify features and label columns\n",
    "    feature_columns = ['Carat', 'Shape_encoded', 'Clarity_encoded', 'Color_encoded', 'Polish_encoded', 'Symmetry_encoded', 'Fluorescence_encoded'] \n",
    "    label_column = 'price_label'\n",
    "\n",
    "    # Instantiate the RandomForestModeler with DataFrames\n",
    "    rf_modeler = RandomForestModeler(train_df=train_df, test_df=test_df)\n",
    "\n",
    "    # Train the Random Forest model\n",
    "    rf_modeler.train_random_forest(feature_cols=feature_columns, label_col=label_column, num_trees=20)\n",
    "    \n",
    "    # Evaluate model metrics\n",
    "    metrics = rf_modeler.evaluate_model(feature_cols=feature_columns, label_col=label_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f6537-afb5-4582-b975-9dfda96279eb",
   "metadata": {},
   "source": [
    "### 2.4(b) Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b22d222-f0cf-481b-a65b-413432a95a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, train_df: DataFrame, test_df: DataFrame, feature_cols: list, label_col: str):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.feature_cols = feature_cols\n",
    "        self.label_col = label_col\n",
    "        \n",
    "        # Assemble features into a single vector column\n",
    "        self.feature_assembler = VectorAssembler(inputCols=self.feature_cols, outputCol='features')\n",
    "        self.train_df = self.feature_assembler.transform(self.train_df)\n",
    "        self.test_df = self.feature_assembler.transform(self.test_df)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def train(self, max_iter: int = 100):\n",
    "        \"\"\"\n",
    "        Train a Gradient Boosted Trees Regressor model.\n",
    "        \"\"\"\n",
    "        gbt = GBTRegressor(\n",
    "            featuresCol='features',\n",
    "            labelCol=self.label_col,\n",
    "            maxIter=max_iter\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        self.model = gbt.fit(self.train_df)\n",
    "        print(\"Gradient Boosting Regressor model trained.\")\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate the Gradient Boosting model using the test dataset.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been trained.\")\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = self.model.transform(self.test_df)\n",
    "        \n",
    "        # Initialize evaluators for different metrics\n",
    "        evaluator_rmse = RegressionEvaluator(labelCol=self.label_col, predictionCol='prediction', metricName='rmse')\n",
    "        evaluator_mae = RegressionEvaluator(labelCol=self.label_col, predictionCol='prediction', metricName='mae')\n",
    "        evaluator_r2 = RegressionEvaluator(labelCol=self.label_col, predictionCol='prediction', metricName='r2')\n",
    "        \n",
    "        # Compute metrics\n",
    "        rmse = evaluator_rmse.evaluate(predictions)\n",
    "        mae = evaluator_mae.evaluate(predictions)\n",
    "        r2 = evaluator_r2.evaluate(predictions)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")\n",
    "        print(f\"Mean Absolute Error (MAE) on test data = {mae}\")\n",
    "        print(f\"R-squared on test data = {r2}\")\n",
    "\n",
    "        # Show some predictions\n",
    "        predictions.select('features', self.label_col, 'prediction').show(10)\n",
    "        \n",
    "        return {\n",
    "            \"rmse\": rmse,\n",
    "            \"mae\": mae,\n",
    "            \"r2\": r2\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_path = \"/user/student/train_data\"\n",
    "    test_path = \"/user/student/test_data\"\n",
    "\n",
    "    train_df = hdfs_reader.read_csv(train_path)\n",
    "    test_df = hdfs_reader.read_csv(test_path)\n",
    "    \n",
    "    # Specify features and label columns\n",
    "    feature_columns = ['Carat', 'Shape_encoded', 'Clarity_encoded', 'Color_encoded', 'Polish_encoded', 'Symmetry_encoded', 'Fluorescence_encoded']\n",
    "    label_column = 'Price'\n",
    "    \n",
    "    # Initialize GradientBoostingRegressor with train and test datasets\n",
    "    gbt_regressor = GradientBoostingRegressor(train_df=train_df, test_df=test_df, feature_cols=feature_columns, label_col=label_column)\n",
    "    \n",
    "    # Train the Gradient Boosting model\n",
    "    gbt_regressor.train(max_iter=100)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    metrics = gbt_regressor.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6694fbc6-86b9-4680-a71a-0b2152811069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "class RandomForestModeler:\n",
    "    def __init__(self, train_path: str, test_path: str):\n",
    "        self.spark = SparkSession.builder.appName(\"RandomForestModeler\").getOrCreate()\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.train_df = self.load_data(self.train_path)\n",
    "        self.test_df = self.load_data(self.test_path)\n",
    "\n",
    "    def load_data(self, path: str):\n",
    "        \"\"\"\n",
    "        Load dataset from the given HDFS path.\n",
    "        \"\"\"\n",
    "        return self.spark.read.csv(path, header=True, inferSchema=True)\n",
    "\n",
    "    def prepare_data(self, df, feature_cols, label_col):\n",
    "        \"\"\"\n",
    "        Prepares the dataset by assembling features into a single vector column.\n",
    "        \n",
    "        :param df: DataFrame containing the dataset.\n",
    "        :param feature_cols: List of column names to be used as features.\n",
    "        :param label_col: Column name to be used as the label.\n",
    "        \"\"\"\n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "        df_prepared = assembler.transform(df).select(\"features\", label_col)\n",
    "        return df_prepared\n",
    "\n",
    "    def train_random_forest(self, feature_cols, label_col, num_trees=10):\n",
    "        \"\"\"\n",
    "        Trains a Random Forest classifier on the training data.\n",
    "        \n",
    "        :param feature_cols: List of columns used as features.\n",
    "        :param label_col: Column used as the label.\n",
    "        :param num_trees: Number of trees in the Random Forest.\n",
    "        \"\"\"\n",
    "        # Prepare training data\n",
    "        train_data = self.prepare_data(self.train_df, feature_cols, label_col)\n",
    "\n",
    "        # Initialize the Random Forest Classifier\n",
    "        rf = RandomForestClassifier(featuresCol=\"features\", labelCol=label_col, numTrees=num_trees)\n",
    "        \n",
    "        # Train the model\n",
    "        self.rf_model = rf.fit(train_data)\n",
    "        print(\"Random Forest model trained.\")\n",
    "\n",
    "    def evaluate_model(self, feature_cols, label_col):\n",
    "        \"\"\"\n",
    "        Evaluates the trained Random Forest model on the test data using accuracy, F1 score, precision, and recall.\n",
    "        \n",
    "        :param feature_cols: List of columns used as features.\n",
    "        :param label_col: Column used as the label.\n",
    "        \"\"\"\n",
    "        # Prepare test data\n",
    "        test_data = self.prepare_data(self.test_df, feature_cols, label_col)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = self.rf_model.transform(test_data)\n",
    "\n",
    "        # Initialize evaluators for different metrics\n",
    "        accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "        f1_evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"f1\")\n",
    "        precision_evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "        recall_evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "\n",
    "        # Compute metrics\n",
    "        accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "        f1_score = f1_evaluator.evaluate(predictions)\n",
    "        precision = precision_evaluator.evaluate(predictions)\n",
    "        recall = recall_evaluator.evaluate(predictions)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "        print(f\"F1 Score: {f1_score:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall\n",
    "        }\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Stops the Spark session.\n",
    "        \"\"\"\n",
    "        self.spark.stop()\n",
    "        print(\"Spark session stopped.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_path = \"/user/student/train_data\"\n",
    "    test_path = \"/user/student/test_data\"\n",
    "\n",
    "    # Instantiate the RandomForestModeler\n",
    "    rf_modeler = RandomForestModeler(train_path, test_path)\n",
    "\n",
    "    # Specify features and label columns\n",
    "    feature_columns = ['Carat','Color_encoded'] \n",
    "    label_column = 'price_label'  \n",
    "\n",
    "    # Train the Random Forest model\n",
    "    rf_modeler.train_random_forest(feature_cols=feature_columns, label_col=label_column, num_trees=20)\n",
    "    \n",
    "    #Metrics\n",
    "    metrics = rf_modeler.evaluate_model(feature_cols=feature_columns, label_col=label_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a6228f-54f3-460b-b3d2-2394e689d1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, log, sqrt\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "    \n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, train_df: DataFrame, test_df: DataFrame, feature_cols: list, label_col: str):\n",
    "        self.train_df = self.create_interaction_features(train_df)\n",
    "        self.test_df = self.create_interaction_features(test_df)\n",
    "        self.feature_cols = feature_cols\n",
    "        self.label_col = label_col\n",
    "        \n",
    "        # Assemble features into a single vector column\n",
    "        self.feature_assembler = VectorAssembler(inputCols=self.feature_cols, outputCol='features')\n",
    "        self.train_df = self.feature_assembler.transform(self.train_df)\n",
    "        self.test_df = self.feature_assembler.transform(self.test_df)\n",
    "\n",
    "    def create_interaction_features(self, df: DataFrame) -> DataFrame:\n",
    "        df = df.withColumn('Carat_Color_Interaction', col('Carat') * col('Color_encoded'))\n",
    "        df = df.withColumn('Carat_Clarity_Interaction', col('Carat') * col('Clarity_encoded'))\n",
    "        df = df.withColumn('Carat_Polish_Interaction', col('Carat') * col('Polish_encoded'))\n",
    "        df = df.withColumn('Carat_Symmetry_Interaction', col('Carat') * col('Symmetry_encoded'))\n",
    "        df = df.withColumn('Carat_Fluorescence_Interaction', col('Carat') * col('Fluorescence_encoded'))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def train(self, max_iter: int = 100):\n",
    "        \"\"\"\n",
    "        Train a Gradient Boosted Trees Regressor model.\n",
    "        \"\"\"\n",
    "        gbt = GBTRegressor(\n",
    "            featuresCol='features',\n",
    "            labelCol=self.label_col,\n",
    "            maxIter=max_iter\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        self.model = gbt.fit(self.train_df)\n",
    "        print(\"Gradient Boosting Regressor model trained.\")\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate the Gradient Boosting model using the test dataset.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been trained.\")\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = self.model.transform(self.test_df)\n",
    "        \n",
    "        # Initialize evaluators for different metrics\n",
    "        evaluator_rmse = RegressionEvaluator(labelCol=self.label_col, predictionCol='prediction', metricName='rmse')\n",
    "        evaluator_mae = RegressionEvaluator(labelCol=self.label_col, predictionCol='prediction', metricName='mae')\n",
    "        evaluator_r2 = RegressionEvaluator(labelCol=self.label_col, predictionCol='prediction', metricName='r2')\n",
    "        \n",
    "        # Compute metrics\n",
    "        rmse = evaluator_rmse.evaluate(predictions)\n",
    "        mae = evaluator_mae.evaluate(predictions)\n",
    "        r2 = evaluator_r2.evaluate(predictions)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")\n",
    "        print(f\"Mean Absolute Error (MAE) on test data = {mae}\")\n",
    "        print(f\"R-squared on test data = {r2}\")\n",
    "\n",
    "        # Show some predictions\n",
    "        predictions.select('features', self.label_col, 'prediction').show(10)\n",
    "        \n",
    "        return {\n",
    "            \"rmse\": rmse,\n",
    "            \"mae\": mae,\n",
    "            \"r2\": r2\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_path = \"/user/student/train_data\"\n",
    "    test_path = \"/user/student/test_data\"\n",
    "\n",
    "    train_df = hdfs_reader.read_csv(train_path)\n",
    "    test_df = hdfs_reader.read_csv(test_path)\n",
    "    \n",
    "    # Specify features and label columns\n",
    "    label_column = 'Price'  \n",
    "    feature_columns = ['Carat', 'Color_encoded', 'Clarity_encoded', 'Polish_encoded', 'Symmetry_encoded', 'Fluorescence_encoded', \n",
    "                       'Carat_Color_Interaction', 'Carat_Clarity_Interaction', 'Carat_Polish_Interaction', 'Carat_Symmetry_Interaction', \n",
    "                       'Carat_Fluorescence_Interaction']\n",
    "    \n",
    "    # Initialize GradientBoostingRegressor with train and test datasets\n",
    "    gbt_regressor = GradientBoostingRegressor(train_df=train_df, test_df=test_df, feature_cols=feature_columns, label_col=label_column)\n",
    "    \n",
    "    # Train the Gradient Boosting model\n",
    "    gbt_regressor.train(max_iter=100)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    metrics = gbt_regressor.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b537b3ed-ca17-4711-bebb-36be696d41a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "de-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
